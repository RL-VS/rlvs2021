# Pitfalls in Policy Gradient methods

## Abstract

In this talk, I will present the behavior of variants of the REINFORCE algorithm using simple gym classic control benchmarks (CartPole, Pendulum, MountainCar...) and various stochastic policy representations (Bernoulli, Gaussian, squashed Gaussian). I will highlight difficulties faced by these algorithms on those simple environments and draw lessons about the necessity to better understanding how they work or why they don't before moving to more advanced methods and more complex benchmarks. 

## Speaker

[Olivier Sigaud](olivier-sigaud.md)

## Class material
<iframe width="560" height="315" src="https://www.youtube.com/embed/U61gsXU4UJc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

Slides [(1/4)](class-material/pg-pitfalls/1_cartpole_working.pdf) [(2/4)](class-material/pg-pitfalls/2_cartpole_issues.pdf) [(3/4)](class-material/pg-pitfalls/3_critic_learning.pdf) [(4/4)](class-material/pg-pitfalls/4_other_benchmarks.pdf)   
[Github repo](https://github.com/osigaud/Basic-Policy-Gradient-Labs)

