# Stochastic bandits

## Abstract

The bandit framework specialises the reinforcement learning setup by removing the (controlled) state. Bandits provide all the essential ingredients to study the exploration/exploitation dilemma, with many principles derived for bandits generalising to the reinforcement learning setting. The simplification has the advantage that it permits a more complete understanding and practical algorithms. Furthermore, bandits are a good model for many applications.

I will introduce bandit problems and present the most well known algorithms based on the principle of optimism in the face of uncertainty. There will be a live coding demo and discussions of the many extensions needed in practical applications.

## Speaker

[Tor Lattimore](tor-lattimore.md)

## Class material

[Slides](class-material/stochastic-bandits-mcts/Lattimore-slides.pdf)   
[Code](class-material/stochastic-bandits-mcts/bandits.zip)   
[The Bandit Algorithms book](https://tor-lattimore.com/downloads/book/book.pdf)   
[Notebook on colab](https://colab.research.google.com/github/RL-VS/rlvs2021/blob/main/docs/class-material/stochastic-bandits-mcts/Stochastic%20Bandits.ipynb) implementing the same code as above.  

