# Stochastic bandits

## Abstract

The bandit framework specializes the reinforcement learning setup by removing the (controlled) state. Bandits provide all the essential ingredients to study the exploration/exploitation dilemma, with many principles derived for bandits generalizing to the reinforcement learning setting. The simplification has the advantage that it permits a more complete understanding and practical algorithms. Furthermore, bandits are a good model for many applications.

I will introduce bandit problems and present the most well-known algorithms based on the principle of optimism in the face of uncertainty. There will be a live coding demo and discussions of the many extensions needed in practical applications.

## Speaker

[Tor Lattimore](tor-lattimore.md)

## Class material

[Slides](class-material/stochastic-bandits-mcts/Lattimore-slides.pdf)   
[Code](class-material/stochastic-bandits-mcts/bandits.zip)   
[The Bandit Algorithms book](https://tor-lattimore.com/downloads/book/book.pdf)   
[Notebook on colab](https://colab.research.google.com/github/RL-VS/rlvs2021/blob/main/docs/class-material/stochastic-bandits-mcts/Stochastic%20Bandits.ipynb) implementing the same code as above.  
<iframe width="560" height="315" src="https://www.youtube.com/embed/Ff23UNTFjGY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

