# Stochastic bandits

## Abstract

The bandit framework specializes the reinforcement learning setup by removing the (controlled) state. Bandits provide all the essential ingredients to study the exploration/exploitation dilemma, with many principles derived for bandits generalizing to the reinforcement learning setting. The simplification has the advantage that it permits a more complete understanding and practical algorithms. Furthermore, bandits are a good model for many applications.

I will introduce bandit problems and present the most well-known algorithms based on the principle of optimism in the face of uncertainty. There will be a live coding demo and discussions of the many extensions needed in practical applications.

## Speaker

[Tor Lattimore](tor-lattimore.md)

## Class material

[Slides](class-material/stochastic-bandits-mcts/Lattimore-slides.pdf)   
[Code](class-material/stochastic-bandits-mcts/bandits.zip)   
[The Bandit Algorithms book](https://tor-lattimore.com/downloads/book/book.pdf)   
[Notebook on colab](https://colab.research.google.com/github/RL-VS/rlvs2021/blob/main/docs/class-material/stochastic-bandits-mcts/Stochastic%20Bandits.ipynb) implementing the same code as above.  
[Video recording](https://us02web.zoom.us/rec/play/uVmfK2G7sCAOziq3WOCMwyKdh2WwIpgrwiJlTce7ZDrcTS3N5VeMXF02hUtjybksDFx5gLcfP7C0CCjx.zJ657mgqkAGK_7Fa?startTime=1616749212000&_x_zm_rtaid=cYqXh5F8Rxqhex-lcA67OA.1616933286464.0a0c1a10574285ccab30e549951279f7&_x_zm_rhtaid=636)


