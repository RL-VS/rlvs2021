{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"RLVS 2021 \ud83d\udd17 Welcome to the 2021 Reinforcement Learning Virtual School's website. This edition is hosted by ANITI . - Event frontpage: https://rlvs.aniti.fr . - Zoom webinar: https://us02web.zoom.us/j/86168201204?pwd=aHBEYllXOHllSmRDUVhZaXhoam9xZz09 - Chat sessions: join here (or follow these instructions if you don't have a Matrix account) There is also an app for registered participants, though we will mostly use Zoom and Matrix. You can download the mobile app or access it through your browser . Goals of RLVS \ud83d\udd17 RLVS intends to provide a high-quality, easy access to the field of Reinforcement Learning to new researchers. It aims to: - Allow attendees to step in to the field of RL - Learn from top lecturers in the field - Provide broad access to live classes We expect the participants to be a mixture of masters and Ph.D. students, academics, and industrial researchers with a solid background in mathematics and computer science. RLVS schedule \ud83d\udd17 This condensed schedule does not include class breaks and social events. Times are Central European Summer Time (UTC+2). Schedule March 25th 9:00-9:10 Opening remarks S. Gerchinovitz 9:10-9:30 RLVS Overview E. Rachelson 9:30-13:00 RL fundamentals E. Rachelson 14:00-16:00 Introduction to Deep Learning D. Wilson 16:30-17:30 Reward Processing Biases in Humans and RL Agents I. Rish 17:45-18:45 Introduction to Hierarchical Reinforcement Learning D. Precup March 26th 10:00-12:00 Stochastic bandits T. Lattimore 14:00-16:00 Monte Carlo Tree Search T. Lattimore 16:30-17:30 Multi-armed bandits in clinical trials D. A. Berry April 1st 9:00-15:00 Deep Q-Networks and its variants B. Piot , C. Tallec 15:15-16:15 Regularized MDPs M. Geist 16:30-17:30 Regret bounds of model-based reinforcement learning M. Wang April 2nd 9:00-12:30 Policy Gradients and Actor Critic methods O. Sigaud 14:00-15:00 Pitfalls in Policy Gradient methods O. Sigaud 15:30-17:30 Exploration in Deep RL M. Pirotta April 8th 9:00-11:00 Evolutionary Reinforcement Learning D. Wilson , J.-B. Mouret 11:30-12:30 Evolving Agents that Learn More Like Animals S. Risi 14:00-16:00 Micro-data Policy Search K. Chatzilygeroudis , J.-B. Mouret 16:30-17:30 Efficient Motor Skills Learning in Robotics D. Lee April 9th 9:00-13:00 RL tips and tricks A. Raffin 14:30-15:30 Symbolic representations and reinforcement learning M. Garnelo 15:45-16:45 Leveraging model-learning for extreme generalization L. P. Kaelbling 17:00-18:00 RLVS wrap-up E. Rachelson , S. Gerchinovitz Speakers \ud83d\udd17 Sorted alphabetically Donald A. Berry University of Texas and Rice University Konstantinos Chatzilygeroudis University of Patras Marta Garnelo DeepMind Matthieu Geist Google Brain Leslie P. Kaelbling MIT Tor Lattimore DeepMind Dongheui Lee Technical University of Munich Jean-Baptiste Mouret INRIA Bilal Piot DeepMind Matteo Pirotta Facebook AI Research Doina Precup McGill University, DeepMind Emmanuel Rachelson ISAE-SUPAERO, Universit\u00e9 de Toulouse Antonin Raffin DLR Irina Rish Universit\u00e9 de Montr\u00e9al Sebastian Risi University of Copenhagen Olivier Sigaud Sorbonne Universit\u00e9 Corentin Tallec DeepMind Mengdi Wang Princeton University Dennis Wilson ISAE-SUPAERO, Universit\u00e9 de Toulouse Organizers \ud83d\udd17 Program committee David Bertoin IRT Saint Exup\u00e9ry Tommaso Cesari Toulouse School of Economics S\u00e9bastien Gerchinovitz IRT Saint Exup\u00e9ry and Institut de Math\u00e9matiques de Toulouse Nicolas Mansard LAAS-CNRS Emmanuel Rachelson ISAE-SUPAERO, Universit\u00e9 de Toulouse Local arrangement chair Corinne Joffre Universit\u00e9 de Toulouse Communication chair Alix Fauque de Jonqui\u00e8res Universit\u00e9 de Toulouse Teaching assistants \ud83d\udd17 Louis B\u00e9thune Etienne de Montbrun Reda Ouhamma Sylvain Roudiere Nguyen Anh Minh MAI Trong-Hieu TRAN Nathalie Neptune Adil Zouitine Pablo Miralles Erwan Lecarpentier Amit Parag S\u00e9bastien Kleff Pierre Laforgue","title":"RLVS home"},{"location":"index.html#rlvs-2021","text":"Welcome to the 2021 Reinforcement Learning Virtual School's website. This edition is hosted by ANITI . - Event frontpage: https://rlvs.aniti.fr . - Zoom webinar: https://us02web.zoom.us/j/86168201204?pwd=aHBEYllXOHllSmRDUVhZaXhoam9xZz09 - Chat sessions: join here (or follow these instructions if you don't have a Matrix account) There is also an app for registered participants, though we will mostly use Zoom and Matrix. You can download the mobile app or access it through your browser .","title":"RLVS 2021"},{"location":"index.html#goals-of-rlvs","text":"RLVS intends to provide a high-quality, easy access to the field of Reinforcement Learning to new researchers. It aims to: - Allow attendees to step in to the field of RL - Learn from top lecturers in the field - Provide broad access to live classes We expect the participants to be a mixture of masters and Ph.D. students, academics, and industrial researchers with a solid background in mathematics and computer science.","title":"Goals of RLVS"},{"location":"index.html#rlvs-schedule","text":"This condensed schedule does not include class breaks and social events. Times are Central European Summer Time (UTC+2). Schedule March 25th 9:00-9:10 Opening remarks S. Gerchinovitz 9:10-9:30 RLVS Overview E. Rachelson 9:30-13:00 RL fundamentals E. Rachelson 14:00-16:00 Introduction to Deep Learning D. Wilson 16:30-17:30 Reward Processing Biases in Humans and RL Agents I. Rish 17:45-18:45 Introduction to Hierarchical Reinforcement Learning D. Precup March 26th 10:00-12:00 Stochastic bandits T. Lattimore 14:00-16:00 Monte Carlo Tree Search T. Lattimore 16:30-17:30 Multi-armed bandits in clinical trials D. A. Berry April 1st 9:00-15:00 Deep Q-Networks and its variants B. Piot , C. Tallec 15:15-16:15 Regularized MDPs M. Geist 16:30-17:30 Regret bounds of model-based reinforcement learning M. Wang April 2nd 9:00-12:30 Policy Gradients and Actor Critic methods O. Sigaud 14:00-15:00 Pitfalls in Policy Gradient methods O. Sigaud 15:30-17:30 Exploration in Deep RL M. Pirotta April 8th 9:00-11:00 Evolutionary Reinforcement Learning D. Wilson , J.-B. Mouret 11:30-12:30 Evolving Agents that Learn More Like Animals S. Risi 14:00-16:00 Micro-data Policy Search K. Chatzilygeroudis , J.-B. Mouret 16:30-17:30 Efficient Motor Skills Learning in Robotics D. Lee April 9th 9:00-13:00 RL tips and tricks A. Raffin 14:30-15:30 Symbolic representations and reinforcement learning M. Garnelo 15:45-16:45 Leveraging model-learning for extreme generalization L. P. Kaelbling 17:00-18:00 RLVS wrap-up E. Rachelson , S. Gerchinovitz","title":"RLVS schedule"},{"location":"index.html#speakers","text":"Sorted alphabetically Donald A. Berry University of Texas and Rice University Konstantinos Chatzilygeroudis University of Patras Marta Garnelo DeepMind Matthieu Geist Google Brain Leslie P. Kaelbling MIT Tor Lattimore DeepMind Dongheui Lee Technical University of Munich Jean-Baptiste Mouret INRIA Bilal Piot DeepMind Matteo Pirotta Facebook AI Research Doina Precup McGill University, DeepMind Emmanuel Rachelson ISAE-SUPAERO, Universit\u00e9 de Toulouse Antonin Raffin DLR Irina Rish Universit\u00e9 de Montr\u00e9al Sebastian Risi University of Copenhagen Olivier Sigaud Sorbonne Universit\u00e9 Corentin Tallec DeepMind Mengdi Wang Princeton University Dennis Wilson ISAE-SUPAERO, Universit\u00e9 de Toulouse","title":"Speakers"},{"location":"index.html#organizers","text":"Program committee David Bertoin IRT Saint Exup\u00e9ry Tommaso Cesari Toulouse School of Economics S\u00e9bastien Gerchinovitz IRT Saint Exup\u00e9ry and Institut de Math\u00e9matiques de Toulouse Nicolas Mansard LAAS-CNRS Emmanuel Rachelson ISAE-SUPAERO, Universit\u00e9 de Toulouse Local arrangement chair Corinne Joffre Universit\u00e9 de Toulouse Communication chair Alix Fauque de Jonqui\u00e8res Universit\u00e9 de Toulouse","title":"Organizers"},{"location":"index.html#teaching-assistants","text":"Louis B\u00e9thune Etienne de Montbrun Reda Ouhamma Sylvain Roudiere Nguyen Anh Minh MAI Trong-Hieu TRAN Nathalie Neptune Adil Zouitine Pablo Miralles Erwan Lecarpentier Amit Parag S\u00e9bastien Kleff Pierre Laforgue","title":"Teaching assistants"},{"location":"alix-fauque-de-jonquieres.html","text":"Alix Fauque de Jonqui\u00e8res \ud83d\udd17","title":"Alix Fauque de Jonqui\u00e8res"},{"location":"alix-fauque-de-jonquieres.html#alix-fauque-de-jonquieres","text":"","title":"Alix Fauque de Jonqui\u00e8res"},{"location":"antonin-raffin.html","text":"Antonin Raffin \ud83d\udd17 Antonin Raffin is a Research Engineer in Robotics and Machine Learning at the German Aerospace Center (DLR). He was previously working on state representation learning in the ENSTA robotics lab (U2IS) where he co-created the Stable-Baselines library with Ashley Hill. His research focus is now on applying reinforcement learning directly on real robots, for which is continue to maintain the Stable-Baselines3 library. Website: https://araffin.github.io/","title":"A. Raffin"},{"location":"antonin-raffin.html#antonin-raffin","text":"Antonin Raffin is a Research Engineer in Robotics and Machine Learning at the German Aerospace Center (DLR). He was previously working on state representation learning in the ENSTA robotics lab (U2IS) where he co-created the Stable-Baselines library with Ashley Hill. His research focus is now on applying reinforcement learning directly on real robots, for which is continue to maintain the Stable-Baselines3 library. Website: https://araffin.github.io/","title":"Antonin Raffin"},{"location":"bilal-piot.html","text":"Bilal Piot \ud83d\udd17 Bilal Piot is a research scientist at Deepmind. He has been working in Imitation Learning, Self-supervised Learning and Reinforcement Learning. He is currently working on exploration strategies in no-reward environments.","title":"B. Piot"},{"location":"bilal-piot.html#bilal-piot","text":"Bilal Piot is a research scientist at Deepmind. He has been working in Imitation Learning, Self-supervised Learning and Reinforcement Learning. He is currently working on exploration strategies in no-reward environments.","title":"Bilal Piot"},{"location":"clinical.html","text":"Multi-armed bandits in clinical trials \ud83d\udd17 Abstract \ud83d\udd17 Bayesian bandit problems have been described in the theoretical statistics literature since 1933. I\u2019ll say how I got into the area as a graduate student and some of my contributions to the bandit theory. My motivation was clinical trials, even though I came to learn that this application was summarily rejected by all clinical trialists \u2026 at the time. I\u2019ll give you a few-minute tour through several decades of my life where I worked within the conventional clinical trial establishment trying to change the paradigm. In the last decade I have had more than a modicum of success. I\u2019ll describe actual national and global clinical trials that I\u2019ve design and that are being conducted today. I call them Bayesian adaptive platform trials, which is a euphemism for multi-armed bandits. They are getting a surprising amount of support from regulators. For example, the Director of the Center for Drugs at the FDA says, \u201cThese have to be the future.\" Speaker \ud83d\udd17 Donald A. Berry Class material \ud83d\udd17 Slides Video recording","title":"Multi-armed bandits in clinical trials"},{"location":"clinical.html#multi-armed-bandits-in-clinical-trials","text":"","title":"Multi-armed bandits in clinical trials"},{"location":"clinical.html#abstract","text":"Bayesian bandit problems have been described in the theoretical statistics literature since 1933. I\u2019ll say how I got into the area as a graduate student and some of my contributions to the bandit theory. My motivation was clinical trials, even though I came to learn that this application was summarily rejected by all clinical trialists \u2026 at the time. I\u2019ll give you a few-minute tour through several decades of my life where I worked within the conventional clinical trial establishment trying to change the paradigm. In the last decade I have had more than a modicum of success. I\u2019ll describe actual national and global clinical trials that I\u2019ve design and that are being conducted today. I call them Bayesian adaptive platform trials, which is a euphemism for multi-armed bandits. They are getting a surprising amount of support from regulators. For example, the Director of the Center for Drugs at the FDA says, \u201cThese have to be the future.\"","title":"Abstract"},{"location":"clinical.html#speaker","text":"Donald A. Berry","title":"Speaker"},{"location":"clinical.html#class-material","text":"Slides Video recording","title":"Class material"},{"location":"corentin-tallec.html","text":"Corentin Tallec \ud83d\udd17 Corentin Tallec is a Research Scientist at DeepMind. His main research interests cover reinforcement learning, self-supervised learning and recurrent neural networks. He is currently working on learning and evaluating interesting behaviors with sparse, or even no reward signals.","title":"C. Tallec"},{"location":"corentin-tallec.html#corentin-tallec","text":"Corentin Tallec is a Research Scientist at DeepMind. His main research interests cover reinforcement learning, self-supervised learning and recurrent neural networks. He is currently working on learning and evaluating interesting behaviors with sparse, or even no reward signals.","title":"Corentin Tallec"},{"location":"corinne-joffre.html","text":"Corinne Joffre \ud83d\udd17","title":"Corinne Joffre"},{"location":"corinne-joffre.html#corinne-joffre","text":"","title":"Corinne Joffre"},{"location":"david-bertoin.html","text":"David Bertoin \ud83d\udd17","title":"David Bertoin"},{"location":"david-bertoin.html#david-bertoin","text":"","title":"David Bertoin"},{"location":"deep-learning.html","text":"Introduction to Deep Learning \ud83d\udd17 Abstract \ud83d\udd17 Deep Learning, a form of machine learning inspired by biological learning, has powered innovations in many fields, including Reinforcement Learning. While artificial neural networks, the core component of deep learning, have been used in artificial intelligence for decades, recent advances in the domain of deep learning such as GPU computation, convolutional layers, and data availability have led to breakthroughs in machine learning, computer vision, and many applications of deep learning such as protein folding. Deep Reinforcement Learning uses deep neural networks as a central component for many algorithms, such as Deep Q Networks and Soft Actor Critic; it is therefore important to understand deep neural networks for Reinforcement Learning. In this session, we will introduce the theoretical foundations of Deep Learning, namely backpropagation, stochastic gradient descent, and layer operations such as convolution. We will apply these concepts in exercises using the PyTorch library on supervised learning examples. Speaker \ud83d\udd17 Dennis Wilson Class material \ud83d\udd17 Class page Video recording","title":"Intro to Deep Learning"},{"location":"deep-learning.html#introduction-to-deep-learning","text":"","title":"Introduction to Deep Learning"},{"location":"deep-learning.html#abstract","text":"Deep Learning, a form of machine learning inspired by biological learning, has powered innovations in many fields, including Reinforcement Learning. While artificial neural networks, the core component of deep learning, have been used in artificial intelligence for decades, recent advances in the domain of deep learning such as GPU computation, convolutional layers, and data availability have led to breakthroughs in machine learning, computer vision, and many applications of deep learning such as protein folding. Deep Reinforcement Learning uses deep neural networks as a central component for many algorithms, such as Deep Q Networks and Soft Actor Critic; it is therefore important to understand deep neural networks for Reinforcement Learning. In this session, we will introduce the theoretical foundations of Deep Learning, namely backpropagation, stochastic gradient descent, and layer operations such as convolution. We will apply these concepts in exercises using the PyTorch library on supervised learning examples.","title":"Abstract"},{"location":"deep-learning.html#speaker","text":"Dennis Wilson","title":"Speaker"},{"location":"deep-learning.html#class-material","text":"Class page Video recording","title":"Class material"},{"location":"dennis-wilson.html","text":"Dennis Wilson \ud83d\udd17 I am an assistant professor in AI and Data Science at ISAE-SUPAERO . My research focuses on bio-inspired artificial intelligence, namely genetic programming, neural networks, and the evolution of learning. I obtained my PhD at the Institut de Recherche en Informatique de Toulouse (IRIT) on the evolution of design principles for artificial neural networks. Prior to that, I worked in the Anyscale Learning For All group in CSAIL, MIT, applying evolutionary strategies and developmental models to the problem of wind farm layout optimization. I\u2019m currently co-organizer of the Supaero Reinforcement Learning Initiative ( SuReLi ), head of the Data & Decision Sciences Master\u2019s program, and am a part of the Decision Systems research group in the Department of Complex Systems Engineering.","title":"D. Wilson"},{"location":"dennis-wilson.html#dennis-wilson","text":"I am an assistant professor in AI and Data Science at ISAE-SUPAERO . My research focuses on bio-inspired artificial intelligence, namely genetic programming, neural networks, and the evolution of learning. I obtained my PhD at the Institut de Recherche en Informatique de Toulouse (IRIT) on the evolution of design principles for artificial neural networks. Prior to that, I worked in the Anyscale Learning For All group in CSAIL, MIT, applying evolutionary strategies and developmental models to the problem of wind farm layout optimization. I\u2019m currently co-organizer of the Supaero Reinforcement Learning Initiative ( SuReLi ), head of the Data & Decision Sciences Master\u2019s program, and am a part of the Decision Systems research group in the Department of Complex Systems Engineering.","title":"Dennis Wilson"},{"location":"doina-precup.html","text":"Doina Precup \ud83d\udd17 Doina Precup teaches at McGill while conducting fundamental research on reinforcement learning, working in particular on AI applications in areas that have a social impact, such as health care. She\u2019s interested in machine decision-making in situations where uncertainty is high. She is a senior fellow of the Canadian Institute for Advanced Research, fellow of the Association for the Advancement of Artificial Intelligence and she also heads the Montreal office of Deepmind. Specialist In: Artificial intelligence, machine learning, reinforcement learning, reasoning and planning under uncertainty, applications. https://www.cs.mcgill.ca/~dprecup/ https://mila.quebec/personne/doina-precup/","title":"D. Precup"},{"location":"doina-precup.html#doina-precup","text":"Doina Precup teaches at McGill while conducting fundamental research on reinforcement learning, working in particular on AI applications in areas that have a social impact, such as health care. She\u2019s interested in machine decision-making in situations where uncertainty is high. She is a senior fellow of the Canadian Institute for Advanced Research, fellow of the Association for the Advancement of Artificial Intelligence and she also heads the Montreal office of Deepmind. Specialist In: Artificial intelligence, machine learning, reinforcement learning, reasoning and planning under uncertainty, applications. https://www.cs.mcgill.ca/~dprecup/ https://mila.quebec/personne/doina-precup/","title":"Doina Precup"},{"location":"donald-berry.html","text":"Donald A. Berry \ud83d\udd17 Donald Berry is the Founder of Berry Consultants, LLC, and a professor in the Department of Biostatistics of the University of Texas M.D. Anderson Cancer Center. He was the founding Chair of this department in 1999. Dr. Berry received his Ph.D. in statistics from Yale University, and previously served on the faculty at the University of Minnesota and at Duke University. He has held endowed faculty positions at Duke University and M.D. Anderson. Dr. Berry is a well-known developer of statistical theory and methodology. He has designed and supervised the conduct of hundreds of innovative clinical trials, including Bayesian adaptive trials in cancer and other diseases. He has developed Bayesian adaptive designs that efficiently use information that accrues over the course of the trial. These trials minimize sample size while increasing the likelihood of detecting therapeutic activity. A principal focus of his research is the use of biomarkers for learning which patients benefit from which therapies, based on molecular markers and phenotype. In particular, he designed and is a co-PI of I-SPY 2 in high-risk early breast cancer, a trial that was the focus of the two lead articles with two editorials in the July 2016 NEJM. He has designed Phase 3 Bayesian adaptive platform clinical trials in glioblastoma, GBM-AGILE, and pancreatic cancer. He has authored many books on statistics and biostatistics, including Bandit Problems: Sequential Allocation of Experiments , Berry DA & Fristedt B, 1985, Chapman-Hall: London, DOI: 10.1007/978-94-015-3711-7, which has more than 1400 citations. He has over 400 peer-reviewed publications. His Google Scholar H-index is 120. He is a Web of Science Group Highly Cited Researcher and a Thomson Reuters Highly Cited Researcher in recognition of ranking among the top 1% of most-cited researchers in Clinical Medicine. He is a Fellow of the American Statistical Association, the Institute of Mathematical Statistics, and the International Society of Bayesian Analysis. He has received numerous research grants from the U.S. NIH and NSF. https://faculty.mdanderson.org/profiles/donald_berry.html https://www.berryconsultants.com/don-berry/","title":"D. A. Berry"},{"location":"donald-berry.html#donald-a-berry","text":"Donald Berry is the Founder of Berry Consultants, LLC, and a professor in the Department of Biostatistics of the University of Texas M.D. Anderson Cancer Center. He was the founding Chair of this department in 1999. Dr. Berry received his Ph.D. in statistics from Yale University, and previously served on the faculty at the University of Minnesota and at Duke University. He has held endowed faculty positions at Duke University and M.D. Anderson. Dr. Berry is a well-known developer of statistical theory and methodology. He has designed and supervised the conduct of hundreds of innovative clinical trials, including Bayesian adaptive trials in cancer and other diseases. He has developed Bayesian adaptive designs that efficiently use information that accrues over the course of the trial. These trials minimize sample size while increasing the likelihood of detecting therapeutic activity. A principal focus of his research is the use of biomarkers for learning which patients benefit from which therapies, based on molecular markers and phenotype. In particular, he designed and is a co-PI of I-SPY 2 in high-risk early breast cancer, a trial that was the focus of the two lead articles with two editorials in the July 2016 NEJM. He has designed Phase 3 Bayesian adaptive platform clinical trials in glioblastoma, GBM-AGILE, and pancreatic cancer. He has authored many books on statistics and biostatistics, including Bandit Problems: Sequential Allocation of Experiments , Berry DA & Fristedt B, 1985, Chapman-Hall: London, DOI: 10.1007/978-94-015-3711-7, which has more than 1400 citations. He has over 400 peer-reviewed publications. His Google Scholar H-index is 120. He is a Web of Science Group Highly Cited Researcher and a Thomson Reuters Highly Cited Researcher in recognition of ranking among the top 1% of most-cited researchers in Clinical Medicine. He is a Fellow of the American Statistical Association, the Institute of Mathematical Statistics, and the International Society of Bayesian Analysis. He has received numerous research grants from the U.S. NIH and NSF. https://faculty.mdanderson.org/profiles/donald_berry.html https://www.berryconsultants.com/don-berry/","title":"Donald A. Berry"},{"location":"dongheui-lee.html","text":"Dongheui Lee \ud83d\udd17 Professor Dongheui Lee is Associate Professor of Human-centered Assistive Robotics at the TUM Department of Electrical and Computer Engineering. She is also director of a Human-centered assistive robotics group at the German Aerospace Center (DLR). Her research interests include human motion understanding, human robot interaction, machine learning in robotics, and assistive robotics. Prior to her appointment as Associate Professor, she was an Assistant Professor at TUM (2009-2017), Project Assistant Professor at the University of Tokyo (2007-2009), and a research scientist at the Korea Institute of Science and Technology (KIST) (2001-2004). After completing her B.S. (2001) and M.S. (2003) degrees in mechanical engineering at Kyung Hee University, Korea, she went on to obtain a PhD degree from the department of Mechano-Informatics, University of Tokyo, Japan in 2007. She was awarded a Carl von Linde Fellowship at the TUM Institute for Advanced Study (2011) and a Helmholtz professorship prize (2015). She is coordinator of both the euRobotics Topic Group on physical Human Robot Interaction and of the TUM Center of Competence Robotics, Autonomy and Interaction. https://www.professoren.tum.de/en/lee-dongheui","title":"D. Lee"},{"location":"dongheui-lee.html#dongheui-lee","text":"Professor Dongheui Lee is Associate Professor of Human-centered Assistive Robotics at the TUM Department of Electrical and Computer Engineering. She is also director of a Human-centered assistive robotics group at the German Aerospace Center (DLR). Her research interests include human motion understanding, human robot interaction, machine learning in robotics, and assistive robotics. Prior to her appointment as Associate Professor, she was an Assistant Professor at TUM (2009-2017), Project Assistant Professor at the University of Tokyo (2007-2009), and a research scientist at the Korea Institute of Science and Technology (KIST) (2001-2004). After completing her B.S. (2001) and M.S. (2003) degrees in mechanical engineering at Kyung Hee University, Korea, she went on to obtain a PhD degree from the department of Mechano-Informatics, University of Tokyo, Japan in 2007. She was awarded a Carl von Linde Fellowship at the TUM Institute for Advanced Study (2011) and a Helmholtz professorship prize (2015). She is coordinator of both the euRobotics Topic Group on physical Human Robot Interaction and of the TUM Center of Competence Robotics, Autonomy and Interaction. https://www.professoren.tum.de/en/lee-dongheui","title":"Dongheui Lee"},{"location":"dqn.html","text":"Deep Q-Networks and its variants \ud83d\udd17 Abstract \ud83d\udd17 We will present in a coherent paradigm the different breakthroughs that led to the seminal paper Deep Q-Networks (DQN). Starting from dynamic programming and the value iteration algorithm, we will show how DQN can be seen as a particular instance of an approximate value iteration algorithm. Then, we will present an open-source codebase released by DeepMind called dqn_zoo that implements in JAX the DQN algorithm. Finally, we will present the variants of DQN, from Double DQN (DDQN) to Implicite Quantile DQN (IQN) and more. Speakers \ud83d\udd17 Bilal Piot Corentin Tallec Prerequisites \ud83d\udd17 Understanding of Markov Decisions Processes and Bellman equations (starting point of the presentation) Basic notions of Python and JAX (Optional) Run the dqn_zoo codebase Class material: \ud83d\udd17 Slides Notebook Video 1 Video 2 (Practical Session) Video 3","title":"Deep Q-Networks and its variants"},{"location":"dqn.html#deep-q-networks-and-its-variants","text":"","title":"Deep Q-Networks and its variants"},{"location":"dqn.html#abstract","text":"We will present in a coherent paradigm the different breakthroughs that led to the seminal paper Deep Q-Networks (DQN). Starting from dynamic programming and the value iteration algorithm, we will show how DQN can be seen as a particular instance of an approximate value iteration algorithm. Then, we will present an open-source codebase released by DeepMind called dqn_zoo that implements in JAX the DQN algorithm. Finally, we will present the variants of DQN, from Double DQN (DDQN) to Implicite Quantile DQN (IQN) and more.","title":"Abstract"},{"location":"dqn.html#speakers","text":"Bilal Piot Corentin Tallec","title":"Speakers"},{"location":"dqn.html#prerequisites","text":"Understanding of Markov Decisions Processes and Bellman equations (starting point of the presentation) Basic notions of Python and JAX (Optional) Run the dqn_zoo codebase","title":"Prerequisites"},{"location":"dqn.html#class-material","text":"Slides Notebook Video 1 Video 2 (Practical Session) Video 3","title":"Class material:"},{"location":"efficient-motor.html","text":"Efficient Motor Skills Learning in Robotics \ud83d\udd17 Abstract \ud83d\udd17 As a fundamental cornerstone in the development of intelligent robotic systems, the research community on robot learning has addressed autonomous motor skill learning and control in complex task scenarios. Imitation learning provides an efficient way to learn new skills through human guidance, which can reduce time and cost to program the robot. Robot learning architectures can provide a comprehensive framework for learning, recognition and reproduction of whole body motions. The inference mechanism can be applied not only to learn the robot's free body motion but also to learn physical interaction tasks, including human robot interaction. In this talk, I will introduce robot learning algorithms including learning from human demonstrations, incremental learning and the extension of learning from simple movement primitives to complex tasks, such as context aware manipulation task, and human robot collaboration. Speaker \ud83d\udd17 Dongheui Lee Class material \ud83d\udd17 Slides","title":"Efficient Motor Skills Learning in Robotics"},{"location":"efficient-motor.html#efficient-motor-skills-learning-in-robotics","text":"","title":"Efficient Motor Skills Learning in Robotics"},{"location":"efficient-motor.html#abstract","text":"As a fundamental cornerstone in the development of intelligent robotic systems, the research community on robot learning has addressed autonomous motor skill learning and control in complex task scenarios. Imitation learning provides an efficient way to learn new skills through human guidance, which can reduce time and cost to program the robot. Robot learning architectures can provide a comprehensive framework for learning, recognition and reproduction of whole body motions. The inference mechanism can be applied not only to learn the robot's free body motion but also to learn physical interaction tasks, including human robot interaction. In this talk, I will introduce robot learning algorithms including learning from human demonstrations, incremental learning and the extension of learning from simple movement primitives to complex tasks, such as context aware manipulation task, and human robot collaboration.","title":"Abstract"},{"location":"efficient-motor.html#speaker","text":"Dongheui Lee","title":"Speaker"},{"location":"efficient-motor.html#class-material","text":"Slides","title":"Class material"},{"location":"emmanuel-rachelson.html","text":"Emmanuel Rachelson \ud83d\udd17 I am a professor of Machine Learning and Optimization at ISAE-SUPAERO . My research lies in the fields of Reinforcement Learning and Sequential Decision problems, with a broader interest for Machine Learning and Operations Research. I am the part of the \"Decision Systems\" research group within the Department of Complex Systems Engineering (DISC) and the founder and co-head of SuReLI (Supaero Reinforcement Learning Initiative) . I previously held positions as a postdoctoral fellow with Pr. M. G. Lagoudakis , then Pr. D. Ernst , and as a permanent researcher at Electricit\u00e9 de France. I created the Data & Decision Sciences MS curriculum and have been implied more generally on (most) AI-related topics within the ISAE-SUPAERO cursus. My current focus is on my research group. My recent work has covered non-negative transfer between RL environments , risk-averse planning , and a few applications at the interface between combinatorial optimization and ML. I am also currently interested in information disentanglement in RL, and neural consolidation processes for lifelong learning. My website: https://personnel.isae-supaero.fr/emmanuel-rachelson","title":"E. Rachelson"},{"location":"emmanuel-rachelson.html#emmanuel-rachelson","text":"I am a professor of Machine Learning and Optimization at ISAE-SUPAERO . My research lies in the fields of Reinforcement Learning and Sequential Decision problems, with a broader interest for Machine Learning and Operations Research. I am the part of the \"Decision Systems\" research group within the Department of Complex Systems Engineering (DISC) and the founder and co-head of SuReLI (Supaero Reinforcement Learning Initiative) . I previously held positions as a postdoctoral fellow with Pr. M. G. Lagoudakis , then Pr. D. Ernst , and as a permanent researcher at Electricit\u00e9 de France. I created the Data & Decision Sciences MS curriculum and have been implied more generally on (most) AI-related topics within the ISAE-SUPAERO cursus. My current focus is on my research group. My recent work has covered non-negative transfer between RL environments , risk-averse planning , and a few applications at the interface between combinatorial optimization and ML. I am also currently interested in information disentanglement in RL, and neural consolidation processes for lifelong learning. My website: https://personnel.isae-supaero.fr/emmanuel-rachelson","title":"Emmanuel Rachelson"},{"location":"evo-rl.html","text":"Evolutionary reinforcement learning \ud83d\udd17 Abstract \ud83d\udd17 Reinforcement learning traditionally takes inspiration from operant conditioning, that is, trial-and-error learning during the lifetime of the agent. However, evolution is another trial-and-error process that is very successful in nature. This process inspired many algorithms that can also solve reinforcement learning problems while using a very different set of metaphors; in that case, learning happens at the phylogenetic timescale, from generation to generation. While evolutionary learning has its strengths, it also raises its own challenges. In this class, we will focus on the representation problem \u201cHow can an artificial genotype describe a neural network that could be as complex as a brain?\u201d\u2014and the stepping stones problem\u2014\u201cwhat intermediate steps lead to an artifact as sophisticated as a brain?\u201d. We will also draw parallels with traditional reinforcement learning methods and attempt to understand the strengths and weaknesses of each family of methods. Speakers \ud83d\udd17 Jean-Baptiste Mouret Dennis Wilson Class material \ud83d\udd17 Introduction Evolutionary Strategies ( Colab ) Multi-Objective Optimization Neuroevolution Beyond the fitness function MAP-Elites tutorial ( Colab ) ( original repo ) Video recording Readings: Designing neural networks through neuroevolution Evolving the Behavior of Machines: From Micro to Macroevolution","title":"Evolutionary Reinforcement Learning"},{"location":"evo-rl.html#evolutionary-reinforcement-learning","text":"","title":"Evolutionary reinforcement learning"},{"location":"evo-rl.html#abstract","text":"Reinforcement learning traditionally takes inspiration from operant conditioning, that is, trial-and-error learning during the lifetime of the agent. However, evolution is another trial-and-error process that is very successful in nature. This process inspired many algorithms that can also solve reinforcement learning problems while using a very different set of metaphors; in that case, learning happens at the phylogenetic timescale, from generation to generation. While evolutionary learning has its strengths, it also raises its own challenges. In this class, we will focus on the representation problem \u201cHow can an artificial genotype describe a neural network that could be as complex as a brain?\u201d\u2014and the stepping stones problem\u2014\u201cwhat intermediate steps lead to an artifact as sophisticated as a brain?\u201d. We will also draw parallels with traditional reinforcement learning methods and attempt to understand the strengths and weaknesses of each family of methods.","title":"Abstract"},{"location":"evo-rl.html#speakers","text":"Jean-Baptiste Mouret Dennis Wilson","title":"Speakers"},{"location":"evo-rl.html#class-material","text":"Introduction Evolutionary Strategies ( Colab ) Multi-Objective Optimization Neuroevolution Beyond the fitness function MAP-Elites tutorial ( Colab ) ( original repo ) Video recording Readings: Designing neural networks through neuroevolution Evolving the Behavior of Machines: From Micro to Macroevolution","title":"Class material"},{"location":"evolving-agents.html","text":"Evolving Agents that Learn More Like Animals \ud83d\udd17 Abstract \ud83d\udd17 Deep neuroevolution, a combination of deep neural networks and evolutionary algorithms, has recently shown to be a competitive alternative to other deep reinforcement learning approaches. In this talk, I will present some of our work on creating agents that evolve to learn complex tasks, such as playing games or controlling robots. I will show that these algorithms do not only allow agents to perform in simple environments but also enable them to (1) learn 3D tasks directly from pixels, (2) learn models of the world for rapid planning, and (3) adapt quickly to task changes through a biologically-inspired form of meta-learning. Speaker \ud83d\udd17 Sebastian Risi Class material \ud83d\udd17 Slides Video recording","title":"Evolving Agents that Learn More Like Animals"},{"location":"evolving-agents.html#evolving-agents-that-learn-more-like-animals","text":"","title":"Evolving Agents that Learn More Like Animals"},{"location":"evolving-agents.html#abstract","text":"Deep neuroevolution, a combination of deep neural networks and evolutionary algorithms, has recently shown to be a competitive alternative to other deep reinforcement learning approaches. In this talk, I will present some of our work on creating agents that evolve to learn complex tasks, such as playing games or controlling robots. I will show that these algorithms do not only allow agents to perform in simple environments but also enable them to (1) learn 3D tasks directly from pixels, (2) learn models of the world for rapid planning, and (3) adapt quickly to task changes through a biologically-inspired form of meta-learning.","title":"Abstract"},{"location":"evolving-agents.html#speaker","text":"Sebastian Risi","title":"Speaker"},{"location":"evolving-agents.html#class-material","text":"Slides Video recording","title":"Class material"},{"location":"exploration.html","text":"Exploration-Exploitation in Reinforcement Learning: Function Approximation and Deep RL \ud83d\udd17 Abstract \ud83d\udd17 One of the major challenges in reinforcement learning (RL) is the trade-off between exploration of the environment to gather information and exploitation of the samples observed so far to execute \"good\" (nearly optimal) actions. In this seminar, we review how exploration techniques are paired with function approximation in continuous state-action spaces. In particular, we will focus on the integration of exploration mechanisms with deep learning techniques. The seminar should provide enough theoretical and algorithmic background to understand existing techniques and possibly devise novel methods. Throughout the talk, we will discuss open problems and possible future research directions. Speaker \ud83d\udd17 Matteo Pirotta Class material \ud83d\udd17 Slides ( original file on the RLGammaZero site )","title":"Exploration in Deep RL"},{"location":"exploration.html#exploration-exploitation-in-reinforcement-learning-function-approximation-and-deep-rl","text":"","title":"Exploration-Exploitation in Reinforcement Learning: Function Approximation and Deep RL"},{"location":"exploration.html#abstract","text":"One of the major challenges in reinforcement learning (RL) is the trade-off between exploration of the environment to gather information and exploitation of the samples observed so far to execute \"good\" (nearly optimal) actions. In this seminar, we review how exploration techniques are paired with function approximation in continuous state-action spaces. In particular, we will focus on the integration of exploration mechanisms with deep learning techniques. The seminar should provide enough theoretical and algorithmic background to understand existing techniques and possibly devise novel methods. Throughout the talk, we will discuss open problems and possible future research directions.","title":"Abstract"},{"location":"exploration.html#speaker","text":"Matteo Pirotta","title":"Speaker"},{"location":"exploration.html#class-material","text":"Slides ( original file on the RLGammaZero site )","title":"Class material"},{"location":"faq.html","text":"Frequently Asked Questions \ud83d\udd17 I couldn't register in time, can I still follow the talks? \ud83d\udd17 Yes. Albeit we cannot accept any more registrations, we will publish the Zoom link to the school on our front page . Through this link, anyone can attend classes even without being registered. The same link and all the school material will also be made available for everyone here . Keep an eye on either of these pages. There we will publish further instructions on how to follow lectures and interact in chat during the event. How do I log in to the chat sessions? Do I need to be a registered participant? \ud83d\udd17 You don't need to be a registered participant for that. Join here (or follow these instructions if you don't have a Matrix account) Can I get a certificate of attendance for RLVS? \ud83d\udd17 We can issue certificates of registration (not attendance, since we cannot really check) for participants who registered through Whova. We will issue them after the school. Will the videos be available after the classes? \ud83d\udd17 We will do our best to upload the videos on the website within 2 to 3 days after each talk (provided we have authorization from the speakers).","title":"FAQ"},{"location":"faq.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq.html#i-couldnt-register-in-time-can-i-still-follow-the-talks","text":"Yes. Albeit we cannot accept any more registrations, we will publish the Zoom link to the school on our front page . Through this link, anyone can attend classes even without being registered. The same link and all the school material will also be made available for everyone here . Keep an eye on either of these pages. There we will publish further instructions on how to follow lectures and interact in chat during the event.","title":"I couldn't register in time, can I still follow the talks?"},{"location":"faq.html#how-do-i-log-in-to-the-chat-sessions-do-i-need-to-be-a-registered-participant","text":"You don't need to be a registered participant for that. Join here (or follow these instructions if you don't have a Matrix account)","title":"How do I log in to the chat sessions? Do I need to be a registered participant?"},{"location":"faq.html#can-i-get-a-certificate-of-attendance-for-rlvs","text":"We can issue certificates of registration (not attendance, since we cannot really check) for participants who registered through Whova. We will issue them after the school.","title":"Can I get a certificate of attendance for RLVS?"},{"location":"faq.html#will-the-videos-be-available-after-the-classes","text":"We will do our best to upload the videos on the website within 2 to 3 days after each talk (provided we have authorization from the speakers).","title":"Will the videos be available after the classes?"},{"location":"hierarchical.html","text":"Introduction to Hierarchical Reinforcement Learning \ud83d\udd17 Abstract \ud83d\udd17 Hierarchical reinforcement learning refers to a class of computational methods that enable artificial agents that train using reinforcement learning to act, learn and plan at different levels of temporal abstraction. In this talk, I will review the main ideas of these computational approaches and present some recent advances in this field. In addition to computational results, I will draw some connections between the algorithms\u2019 hierarchical reinforcement learning approaches and existing similar models of human and animal decision making. Speaker \ud83d\udd17 Doina Precup Class material \ud83d\udd17 Video recording","title":"Introduction to Hierarchical Reinforcement Learning"},{"location":"hierarchical.html#introduction-to-hierarchical-reinforcement-learning","text":"","title":"Introduction to Hierarchical Reinforcement Learning"},{"location":"hierarchical.html#abstract","text":"Hierarchical reinforcement learning refers to a class of computational methods that enable artificial agents that train using reinforcement learning to act, learn and plan at different levels of temporal abstraction. In this talk, I will review the main ideas of these computational approaches and present some recent advances in this field. In addition to computational results, I will draw some connections between the algorithms\u2019 hierarchical reinforcement learning approaches and existing similar models of human and animal decision making.","title":"Abstract"},{"location":"hierarchical.html#speaker","text":"Doina Precup","title":"Speaker"},{"location":"hierarchical.html#class-material","text":"Video recording","title":"Class material"},{"location":"human-behavioral-agents.html","text":"Reward Processing Biases in Humans and RL Agents \ud83d\udd17 Abstract \ud83d\udd17 Drawing inspirations from studies of human behavior, we propose a general and flexible parametric framework for sequential decision-making based on a two-stream mechanism for processing positive and negative rewards. Our framework extends standard problem settings, such as multi-armed bandits (MAB), contextual bandits (CB) and general reinforcement learning (RL), allowing to incorporate a wide range of reward-processing biases -- an important component of human decision making which can help us better understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems, as well as various neuropsychiatric conditions associated with disruptions in normal reward processing. The reward processing biases are modeled by a combination of weights on incoming two-stream rewards, as well as on memories about the prior reward history, resulting into more flexible parametric approaches that can outperform standard algorithms for sequential decision making, such as, for example, Q-Learning and SARSA methods, as well as recently proposed Double Q-Learning, on a variety of simulated and realistic tasks. Speaker \ud83d\udd17 Irina Rish Class material \ud83d\udd17 Slides Video recording","title":"Reward Processing Biases in Humans and RL Agents"},{"location":"human-behavioral-agents.html#reward-processing-biases-in-humans-and-rl-agents","text":"","title":"Reward Processing Biases in Humans and RL Agents"},{"location":"human-behavioral-agents.html#abstract","text":"Drawing inspirations from studies of human behavior, we propose a general and flexible parametric framework for sequential decision-making based on a two-stream mechanism for processing positive and negative rewards. Our framework extends standard problem settings, such as multi-armed bandits (MAB), contextual bandits (CB) and general reinforcement learning (RL), allowing to incorporate a wide range of reward-processing biases -- an important component of human decision making which can help us better understand a wide spectrum of multi-agent interactions in complex real-world socioeconomic systems, as well as various neuropsychiatric conditions associated with disruptions in normal reward processing. The reward processing biases are modeled by a combination of weights on incoming two-stream rewards, as well as on memories about the prior reward history, resulting into more flexible parametric approaches that can outperform standard algorithms for sequential decision making, such as, for example, Q-Learning and SARSA methods, as well as recently proposed Double Q-Learning, on a variety of simulated and realistic tasks.","title":"Abstract"},{"location":"human-behavioral-agents.html#speaker","text":"Irina Rish","title":"Speaker"},{"location":"human-behavioral-agents.html#class-material","text":"Slides Video recording","title":"Class material"},{"location":"irina-rish.html","text":"Irina Rish \ud83d\udd17 Irina Rish is an Associate Professor in the Computer Science and Operations Research department at the Universit\u00e9 de Montr\u00e9al (UdeM) and a core member of Mila \u2013 Quebec AI Institute. She holds the Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI. She holds MSc and PhD in AI from University of California, Irvine and MSc in Applied Mathematics from Moscow Gubkin Institute. Dr. Rish\u2019s research focus is on machine learning, neural data analysis and neuroscience-inspired AI. Her current research interests include continual lifelong learning, optimization algorithms for deep neural networks, sparse modelling and probabilistic inference, dialog generation, biologically plausible reinforcement learning, and dynamical systems approaches to brain imaging analysis. Before joining UdeM and Mila in 2019, she was a research scientist at the IBM T. J. Watson Research Center, where she worked on various projects at the intersection of neuroscience and AI, and led the Neuro-AI challenge. Dr. Rish holds 64 patents, has published over 80 research papers, several book chapters, three edited books, and a monograph on Sparse Modelling. https://mila.quebec/en/person/irina-rish/ https://sites.google.com/site/irinarish/","title":"I. Rish"},{"location":"irina-rish.html#irina-rish","text":"Irina Rish is an Associate Professor in the Computer Science and Operations Research department at the Universit\u00e9 de Montr\u00e9al (UdeM) and a core member of Mila \u2013 Quebec AI Institute. She holds the Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI. She holds MSc and PhD in AI from University of California, Irvine and MSc in Applied Mathematics from Moscow Gubkin Institute. Dr. Rish\u2019s research focus is on machine learning, neural data analysis and neuroscience-inspired AI. Her current research interests include continual lifelong learning, optimization algorithms for deep neural networks, sparse modelling and probabilistic inference, dialog generation, biologically plausible reinforcement learning, and dynamical systems approaches to brain imaging analysis. Before joining UdeM and Mila in 2019, she was a research scientist at the IBM T. J. Watson Research Center, where she worked on various projects at the intersection of neuroscience and AI, and led the Neuro-AI challenge. Dr. Rish holds 64 patents, has published over 80 research papers, several book chapters, three edited books, and a monograph on Sparse Modelling. https://mila.quebec/en/person/irina-rish/ https://sites.google.com/site/irinarish/","title":"Irina Rish"},{"location":"jean-baptiste-mouret.html","text":"Jean-Baptiste Mouret \ud83d\udd17 Dr. Jean-Baptiste Mouret is a senior researcher (\"Directeur de recherche\") at Inria, the French research institute dedicated to computer science and mathematics. From 2009 to 2015, he was an assistant professor (\"ma\u00eetre de conf\u00e9rences\") at the Sorbonne University (former Pierre and Marie Curie University, in Paris, France). Overall, J.-B. Mouret conducts researches that intertwine data-efficient machine learning and evolutionary computation to make robots that can adapt in a few minutes. His work was funded by the ERC (project ResiBots), featured on the cover of Nature (\"Robots that adapt like animals\", Cully et al., 2015) and it received several national and international scientific awards, including the \"Prix La Recherche 2016\" and the \"Distinguished Young Investigator in Artificial Life 2017\". https://members.loria.fr/JBMouret/index.html","title":"J.-B. Mouret"},{"location":"jean-baptiste-mouret.html#jean-baptiste-mouret","text":"Dr. Jean-Baptiste Mouret is a senior researcher (\"Directeur de recherche\") at Inria, the French research institute dedicated to computer science and mathematics. From 2009 to 2015, he was an assistant professor (\"ma\u00eetre de conf\u00e9rences\") at the Sorbonne University (former Pierre and Marie Curie University, in Paris, France). Overall, J.-B. Mouret conducts researches that intertwine data-efficient machine learning and evolutionary computation to make robots that can adapt in a few minutes. His work was funded by the ERC (project ResiBots), featured on the cover of Nature (\"Robots that adapt like animals\", Cully et al., 2015) and it received several national and international scientific awards, including the \"Prix La Recherche 2016\" and the \"Distinguished Young Investigator in Artificial Life 2017\". https://members.loria.fr/JBMouret/index.html","title":"Jean-Baptiste Mouret"},{"location":"konstantinos-chatzilygeroudis.html","text":"Konstantinos Chatzilygeroudis \ud83d\udd17 I am currently a Post-doctoral fellow in Machine Learning and Robotics at the Computational Intelligence Laboratory , Department of Mathematics, University of Patras, in collaboration with Prof. Michael N. Vrahatis . My work focuses on using evolutionary and machine learning methods combined with simulations to develop trial-and-error methods that work on physical robots. I am also the Leader of the R&D Computer Vision Team at Metargus , a pre-seed funded start up (based in Patras, Greece), building a cutting-edge basketball coaching tool to provide coaches with insights far beyond traditional analytics. My work focuses on combining traditional Computer Vision techniques with modern Machine/Deep Learning algorithms. I am also currently teaching the under-graduate courses \"Introduction to Artificial Intelligence\" and \"Robotics and Intelligent Agents\" at the Computer Engineering & Informatics Department of University of Patras. http://costashatz.github.io/","title":"K. Chatzilygeroudis"},{"location":"konstantinos-chatzilygeroudis.html#konstantinos-chatzilygeroudis","text":"I am currently a Post-doctoral fellow in Machine Learning and Robotics at the Computational Intelligence Laboratory , Department of Mathematics, University of Patras, in collaboration with Prof. Michael N. Vrahatis . My work focuses on using evolutionary and machine learning methods combined with simulations to develop trial-and-error methods that work on physical robots. I am also the Leader of the R&D Computer Vision Team at Metargus , a pre-seed funded start up (based in Patras, Greece), building a cutting-edge basketball coaching tool to provide coaches with insights far beyond traditional analytics. My work focuses on combining traditional Computer Vision techniques with modern Machine/Deep Learning algorithms. I am also currently teaching the under-graduate courses \"Introduction to Artificial Intelligence\" and \"Robotics and Intelligent Agents\" at the Computer Engineering & Informatics Department of University of Patras. http://costashatz.github.io/","title":"Konstantinos Chatzilygeroudis"},{"location":"leslie-kaelbling.html","text":"Leslie Pack Kaelbling \ud83d\udd17 Leslie is a Professor at MIT. She has an undergraduate degree in Philosophy and a PhD in Computer Science from Stanford, and was previously on the faculty at Brown University. She was the founding editor-in-chief of the Journal of Machine Learning Research. Her goal is to make robots that are as smart as you are. She is a fellow of the AAAI. people.csail.mit.edu/lpk","title":"L. P. Kaelbling"},{"location":"leslie-kaelbling.html#leslie-pack-kaelbling","text":"Leslie is a Professor at MIT. She has an undergraduate degree in Philosophy and a PhD in Computer Science from Stanford, and was previously on the faculty at Brown University. She was the founding editor-in-chief of the Journal of Machine Learning Research. Her goal is to make robots that are as smart as you are. She is a fellow of the AAAI. people.csail.mit.edu/lpk","title":"Leslie Pack Kaelbling"},{"location":"marta-garnelo.html","text":"Marta Garnelo \ud83d\udd17 Marta is a senior research scientist at DeepMind where she has primarily worked on deep generative models and meta learning. As part of this research she has been involved in developing Generative Query Networks and led the work on Neural Processes. In addition to generative models her recent interests have expanded to multi-agent systems and game theory. Prior to DeepMind Marta obtained her PhD from Imperial College London, where she also worked on symbolic reinforcement learning.","title":"M. Garnelo"},{"location":"marta-garnelo.html#marta-garnelo","text":"Marta is a senior research scientist at DeepMind where she has primarily worked on deep generative models and meta learning. As part of this research she has been involved in developing Generative Query Networks and led the work on Neural Processes. In addition to generative models her recent interests have expanded to multi-agent systems and game theory. Prior to DeepMind Marta obtained her PhD from Imperial College London, where she also worked on symbolic reinforcement learning.","title":"Marta Garnelo"},{"location":"matteo-pirotta.html","text":"Matteo Pirotta \ud83d\udd17 I am research scientist at Facebook AI Research in Paris. Previously, I was postdoc at INRIA Lille - Nord Europe in the SequeL team for almost two years. Before I was postdoc at Politecnico di Milano . I have received my PhD in computer science at Politecnico di Milano, under the supervision of Luca Bascetta and Marcello Restelli . My research interest is machine learning. In particular I am interested in reinforcement learning, transfer learning and online learning. https://teopir.github.io/","title":"M. Pirotta"},{"location":"matteo-pirotta.html#matteo-pirotta","text":"I am research scientist at Facebook AI Research in Paris. Previously, I was postdoc at INRIA Lille - Nord Europe in the SequeL team for almost two years. Before I was postdoc at Politecnico di Milano . I have received my PhD in computer science at Politecnico di Milano, under the supervision of Luca Bascetta and Marcello Restelli . My research interest is machine learning. In particular I am interested in reinforcement learning, transfer learning and online learning. https://teopir.github.io/","title":"Matteo Pirotta"},{"location":"matthieu-geist.html","text":"Matthieu Geist \ud83d\udd17 Matthieu Geist obtained an Electrical Engineering degree and an MSc degree in Applied Mathematics in Sept. 2006 (Sup\u00e9lec, France), a PhD degree in Applied Mathematics in Nov. 2009 (University Paul Verlaine of Metz, France) and a Habilitation degree in Feb. 2016 (University Lille 1, France). Between Feb. 2010 and Sept. 2017, he was an assistant professor at CentraleSup\u00e9lec, France. In Sept. 2017, he joined University of Lorraine, France, as a full professor in Applied Mathematics (Interdisciplinary Laboratory for Continental Environments, CNRS-UL). Since Sept. 2018, he is on secondment at Google Brain, as a research scientist (Paris, France). His research interests include machine learning, especially reinforcement learning and imitation learning. https://research.google/people/106211/ Google scholar","title":"M. Geist"},{"location":"matthieu-geist.html#matthieu-geist","text":"Matthieu Geist obtained an Electrical Engineering degree and an MSc degree in Applied Mathematics in Sept. 2006 (Sup\u00e9lec, France), a PhD degree in Applied Mathematics in Nov. 2009 (University Paul Verlaine of Metz, France) and a Habilitation degree in Feb. 2016 (University Lille 1, France). Between Feb. 2010 and Sept. 2017, he was an assistant professor at CentraleSup\u00e9lec, France. In Sept. 2017, he joined University of Lorraine, France, as a full professor in Applied Mathematics (Interdisciplinary Laboratory for Continental Environments, CNRS-UL). Since Sept. 2018, he is on secondment at Google Brain, as a research scientist (Paris, France). His research interests include machine learning, especially reinforcement learning and imitation learning. https://research.google/people/106211/ Google scholar","title":"Matthieu Geist"},{"location":"mcts.html","text":"Monte Carlo Tree Search \ud83d\udd17 Abstract \ud83d\udd17 Monte Carlo Tree Search is an algorithm for game tree search most famous for its application in AlphaGo. I will give a tutorial on this algorithm, which will include a significant practical component in Python. A very basic understanding of Python and Numpy will be useful for students wishing to complete the practical component independently. Speaker \ud83d\udd17 Tor Lattimore Class material \ud83d\udd17 Slides (starts at slide 41) Code Notebook on colab implementing the same code as above with solutions to the exercices Video recording","title":"Monte Carlo Tree Search"},{"location":"mcts.html#monte-carlo-tree-search","text":"","title":"Monte Carlo Tree Search"},{"location":"mcts.html#abstract","text":"Monte Carlo Tree Search is an algorithm for game tree search most famous for its application in AlphaGo. I will give a tutorial on this algorithm, which will include a significant practical component in Python. A very basic understanding of Python and Numpy will be useful for students wishing to complete the practical component independently.","title":"Abstract"},{"location":"mcts.html#speaker","text":"Tor Lattimore","title":"Speaker"},{"location":"mcts.html#class-material","text":"Slides (starts at slide 41) Code Notebook on colab implementing the same code as above with solutions to the exercices Video recording","title":"Class material"},{"location":"mengdi-wang.html","text":"Mendgi Wang \ud83d\udd17 Mengdi Wang is an associate professor at the Department of Electrical Engineering and Center for Statistics and Machine Learning at Princeton University. She is also affiliated with the Department of Computer Science and a visiting research scientist at DeepMind. Her research focuses on data-driven stochastic optimization and applications in machine and reinforcement learning. She received her PhD in Electrical Engineering and Computer Science from Massachusetts Institute of Technology in 2013. At MIT, Mengdi was affiliated with the Laboratory for Information and Decision Systems and was advised by Dimitri P. Bertsekas. Mengdi received the Young Researcher Prize in Continuous Optimization of the Mathematical Optimization Society in 2016 (awarded once every three years), the Princeton SEAS Innovation Award in 2016, the NSF Career Award in 2017, the Google Faculty Award in 2017, and the MIT Tech Review 35-Under-35 Innovation Award (China region) in 2018. She serves as an associate editor for Operations Research and Mathematics of Operations Research, as area chair for ICML, NeurIPS, AISTATS, and is on the editorial board of Journal of Machine Learning Research. Research supported by NSF, AFOSR, NIH, ONR, Google, Microsoft C3.ai DTI, FinUP. https://mwang.princeton.edu/","title":"M. Wang"},{"location":"mengdi-wang.html#mendgi-wang","text":"Mengdi Wang is an associate professor at the Department of Electrical Engineering and Center for Statistics and Machine Learning at Princeton University. She is also affiliated with the Department of Computer Science and a visiting research scientist at DeepMind. Her research focuses on data-driven stochastic optimization and applications in machine and reinforcement learning. She received her PhD in Electrical Engineering and Computer Science from Massachusetts Institute of Technology in 2013. At MIT, Mengdi was affiliated with the Laboratory for Information and Decision Systems and was advised by Dimitri P. Bertsekas. Mengdi received the Young Researcher Prize in Continuous Optimization of the Mathematical Optimization Society in 2016 (awarded once every three years), the Princeton SEAS Innovation Award in 2016, the NSF Career Award in 2017, the Google Faculty Award in 2017, and the MIT Tech Review 35-Under-35 Innovation Award (China region) in 2018. She serves as an associate editor for Operations Research and Mathematics of Operations Research, as area chair for ICML, NeurIPS, AISTATS, and is on the editorial board of Journal of Machine Learning Research. Research supported by NSF, AFOSR, NIH, ONR, Google, Microsoft C3.ai DTI, FinUP. https://mwang.princeton.edu/","title":"Mendgi Wang"},{"location":"micro-data.html","text":"Micro-data policy search \ud83d\udd17 Abstract \ud83d\udd17 Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible when experiments takes time or are expensive (for instance, with physical robot or with an aerodynamics simulator). This class focuses on the extreme other end of the spectrum: how can an algorithm adapt a policy with only a handful of trials (a dozen) and a few minutes? By analogy with the word \"big-data\", we refer to this challenge as \"micro-data reinforcement learning\". We will describe two main strategies: (1) leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators), and (2) create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Most of the examples will be about robotic systems, but the principle apply to any other expensive setup. Speakers \ud83d\udd17 Jean-Baptiste Mouret Konstantinos Chatzilygeroudis Class material \ud83d\udd17 Video recording Slides: Introduction Priors on policy structures Bayesian Optimization Model based policy search Demos: On Bayesian Optimization ( colab link ) ( original repo ). On policy search priors ( original repo ). Readings: A survey on policy search algorithms for learning robot controllers in a handful of trials","title":"Micro-data Policy Search"},{"location":"micro-data.html#micro-data-policy-search","text":"","title":"Micro-data policy search"},{"location":"micro-data.html#abstract","text":"Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible when experiments takes time or are expensive (for instance, with physical robot or with an aerodynamics simulator). This class focuses on the extreme other end of the spectrum: how can an algorithm adapt a policy with only a handful of trials (a dozen) and a few minutes? By analogy with the word \"big-data\", we refer to this challenge as \"micro-data reinforcement learning\". We will describe two main strategies: (1) leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators), and (2) create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Most of the examples will be about robotic systems, but the principle apply to any other expensive setup.","title":"Abstract"},{"location":"micro-data.html#speakers","text":"Jean-Baptiste Mouret Konstantinos Chatzilygeroudis","title":"Speakers"},{"location":"micro-data.html#class-material","text":"Video recording Slides: Introduction Priors on policy structures Bayesian Optimization Model based policy search Demos: On Bayesian Optimization ( colab link ) ( original repo ). On policy search priors ( original repo ). Readings: A survey on policy search algorithms for learning robot controllers in a handful of trials","title":"Class material"},{"location":"model-learning.html","text":"Leveraging model-learning for extreme generalization \ud83d\udd17 Abstract \ud83d\udd17 Early RL was almost completely focused on learning a policy, sometimes via a value function, for a fixed reward function. If the reward function is really fixed and if the agent really needs a quick reaction in all world states, that's the right strategy. But those conditions don't always hold. \"Goal-conditioned\" RL attempts to relax the first condition by allowing the reward function to be, in essence, part of the state. That addresses the first problem but makes the second one much worse! Now we are have to find a quick reaction to all state-goal pairs. I will extol the virtues of learning models instead, with a focus on models that generalize extremely aggressively via factoring and quantification, and argue informally that such models can be learned with relatively less computation and data, and can be acquired incrementally without interference. \"Compilation\" methods can still be used to make reactions fast for very common cases while retaining the ability to deliberate to solve unusual or difficult problems. Depending on time and interest, I might also talk about the substantial partial observability that arises in long-term household robot problems, discuss some of our approaches to it, and encourage participants to help think of others. Speaker \ud83d\udd17 Leslie Kaelbling Class material \ud83d\udd17 Blog post","title":"Leveraging model-learning for extreme generalization"},{"location":"model-learning.html#leveraging-model-learning-for-extreme-generalization","text":"","title":"Leveraging model-learning for extreme generalization"},{"location":"model-learning.html#abstract","text":"Early RL was almost completely focused on learning a policy, sometimes via a value function, for a fixed reward function. If the reward function is really fixed and if the agent really needs a quick reaction in all world states, that's the right strategy. But those conditions don't always hold. \"Goal-conditioned\" RL attempts to relax the first condition by allowing the reward function to be, in essence, part of the state. That addresses the first problem but makes the second one much worse! Now we are have to find a quick reaction to all state-goal pairs. I will extol the virtues of learning models instead, with a focus on models that generalize extremely aggressively via factoring and quantification, and argue informally that such models can be learned with relatively less computation and data, and can be acquired incrementally without interference. \"Compilation\" methods can still be used to make reactions fast for very common cases while retaining the ability to deliberate to solve unusual or difficult problems. Depending on time and interest, I might also talk about the substantial partial observability that arises in long-term household robot problems, discuss some of our approaches to it, and encourage participants to help think of others.","title":"Abstract"},{"location":"model-learning.html#speaker","text":"Leslie Kaelbling","title":"Speaker"},{"location":"model-learning.html#class-material","text":"Blog post","title":"Class material"},{"location":"nicolas-mansard.html","text":"Nicolas Mansard \ud83d\udd17","title":"Nicolas Mansard"},{"location":"nicolas-mansard.html#nicolas-mansard","text":"","title":"Nicolas Mansard"},{"location":"olivier-sigaud.html","text":"Olivier Sigaud \ud83d\udd17 Biography \ud83d\udd17 (1996) PhD in Computer Science \"Learning : from control to behavior\", advisor: Dominique Luzeaux (DGA) (1995-2001) Research engineer, Departement for Advanced Studies, at DASSAULT AVIATION (Aerospace company). (2001-2005) Lecturer in Computer Science UPMC-Paris6 (LIP6, AnimatLab) (2002) PhD in Philosophy \" Automatisme et subjectivit\u00e9 : l'anticipation au coeur de l'exp\u00e9rience\", advisor: Jacques Dubucs (IHSPST) (2004) HDR in Computer Science \"Adaptive Behavior for Agents in Complex Software Settings\" (in french), UPMC-Paris 6 (2005-2006) Professor in Computer Science UPMC-Paris6 (LIP6, AnimatLab) (2007- ) Professor in Computer Science UPMC-Paris6 (ISIR, AMAC team) Research Activities \ud83d\udd17 Understanding intelligence is a fascinating research topic. For a long time, artificial intelligence researchers have focused on typically human intellectual activities such as playing chess, having a conversation or proving theorems. But it is more and more obvious that these specific capabilities are anchored into apparently simpler capabilities such as using objects or tools, displaying social signals through the body posture or more simply moving one's body to interact appropriately with the environment. Performing robotics research reveals the difficulty of these problems that we solve effortlessly. Rather than addressing them through the standard engineering approach, it seems appropriate to try to figure out how our brain solves them. This amounts to investigating the psychological mechanisms through which kids acquire their motor and cognitive capabilities and neurophysiological mechanisms the brain uses to implement this learning process. In this context, my research is focused on designing and exploiting machine learning techniques dedicated to growing motor and cognitive capabilities in robots, and to modelling these capabilities in living beings. In practice, my work belongs to three domains: machine learning, where I'm interested in regression, reinforcement learning, stochastic optimisation and the combination of these methods. This domain provides the fundamental tools for the other two activities; developmental robotics, which strive to endow robots with psychological mechanisms similar to those of kids; computational neurosciences, which strive to design computational models of the neurophysiological mechanism of animal learning. Within ISIR, I am responsible of a group about Learning for Control and Decision in Robotics (four permanent researchers) Up to 2014, I have been in charge of the activities about learning in the Robotics and Neuroscience group of the french working group in Robotics . I have been the principal investigator of the MACSi project (up to april 2014) based on our iCub humanoid robot. I also participate to the CODYCO and the DREAM european projects. Teaching \ud83d\udd17 A page about my teaching activities, with slides, tutorial texts and internship topics is available here .","title":"O. Sigaud"},{"location":"olivier-sigaud.html#olivier-sigaud","text":"","title":"Olivier Sigaud"},{"location":"olivier-sigaud.html#biography","text":"(1996) PhD in Computer Science \"Learning : from control to behavior\", advisor: Dominique Luzeaux (DGA) (1995-2001) Research engineer, Departement for Advanced Studies, at DASSAULT AVIATION (Aerospace company). (2001-2005) Lecturer in Computer Science UPMC-Paris6 (LIP6, AnimatLab) (2002) PhD in Philosophy \" Automatisme et subjectivit\u00e9 : l'anticipation au coeur de l'exp\u00e9rience\", advisor: Jacques Dubucs (IHSPST) (2004) HDR in Computer Science \"Adaptive Behavior for Agents in Complex Software Settings\" (in french), UPMC-Paris 6 (2005-2006) Professor in Computer Science UPMC-Paris6 (LIP6, AnimatLab) (2007- ) Professor in Computer Science UPMC-Paris6 (ISIR, AMAC team)","title":"Biography"},{"location":"olivier-sigaud.html#research-activities","text":"Understanding intelligence is a fascinating research topic. For a long time, artificial intelligence researchers have focused on typically human intellectual activities such as playing chess, having a conversation or proving theorems. But it is more and more obvious that these specific capabilities are anchored into apparently simpler capabilities such as using objects or tools, displaying social signals through the body posture or more simply moving one's body to interact appropriately with the environment. Performing robotics research reveals the difficulty of these problems that we solve effortlessly. Rather than addressing them through the standard engineering approach, it seems appropriate to try to figure out how our brain solves them. This amounts to investigating the psychological mechanisms through which kids acquire their motor and cognitive capabilities and neurophysiological mechanisms the brain uses to implement this learning process. In this context, my research is focused on designing and exploiting machine learning techniques dedicated to growing motor and cognitive capabilities in robots, and to modelling these capabilities in living beings. In practice, my work belongs to three domains: machine learning, where I'm interested in regression, reinforcement learning, stochastic optimisation and the combination of these methods. This domain provides the fundamental tools for the other two activities; developmental robotics, which strive to endow robots with psychological mechanisms similar to those of kids; computational neurosciences, which strive to design computational models of the neurophysiological mechanism of animal learning. Within ISIR, I am responsible of a group about Learning for Control and Decision in Robotics (four permanent researchers) Up to 2014, I have been in charge of the activities about learning in the Robotics and Neuroscience group of the french working group in Robotics . I have been the principal investigator of the MACSi project (up to april 2014) based on our iCub humanoid robot. I also participate to the CODYCO and the DREAM european projects.","title":"Research Activities"},{"location":"olivier-sigaud.html#teaching","text":"A page about my teaching activities, with slides, tutorial texts and internship topics is available here .","title":"Teaching"},{"location":"opening.html","text":"Opening remarks \ud83d\udd17 Abstract \ud83d\udd17 This short introduction will present the organization of RLVS 2021. Speaker \ud83d\udd17 S. Gerchinovitz Material \ud83d\udd17 Video recording Slides","title":"Opening remarks"},{"location":"opening.html#opening-remarks","text":"","title":"Opening remarks"},{"location":"opening.html#abstract","text":"This short introduction will present the organization of RLVS 2021.","title":"Abstract"},{"location":"opening.html#speaker","text":"S. Gerchinovitz","title":"Speaker"},{"location":"opening.html#material","text":"Video recording Slides","title":"Material"},{"location":"organizers.html","text":"Organizers \ud83d\udd17 Program committee David Bertoin IRT Saint Exup\u00e9ry Tommaso Cesari Toulouse School of Economics S\u00e9bastien Gerchinovitz IRT Saint Exup\u00e9ry and Institut de Math\u00e9matiques de Toulouse Nicolas Mansard LAAS-CNRS Emmanuel Rachelson ISAE-SUPAERO Local arrangement chair Corinne Joffre Universit\u00e9 de Toulouse Communication chair Alix Fauque de Jonqui\u00e8res Universit\u00e9 de Toulouse","title":"Organizers"},{"location":"organizers.html#organizers","text":"Program committee David Bertoin IRT Saint Exup\u00e9ry Tommaso Cesari Toulouse School of Economics S\u00e9bastien Gerchinovitz IRT Saint Exup\u00e9ry and Institut de Math\u00e9matiques de Toulouse Nicolas Mansard LAAS-CNRS Emmanuel Rachelson ISAE-SUPAERO Local arrangement chair Corinne Joffre Universit\u00e9 de Toulouse Communication chair Alix Fauque de Jonqui\u00e8res Universit\u00e9 de Toulouse","title":"Organizers"},{"location":"pg-pitfalls.html","text":"Pitfalls in Policy Gradient methods \ud83d\udd17 Abstract \ud83d\udd17 In this talk, I will present the behavior of variants of the REINFORCE algorithm using simple gym classic control benchmarks (CartPole, Pendulum, MountainCar...) and various stochastic policy representations (Bernoulli, Gaussian, squashed Gaussian). I will highlight difficulties faced by these algorithms on those simple environments and draw lessons about the necessity to better understanding how they work or why they don't before moving to more advanced methods and more complex benchmarks. Speaker \ud83d\udd17 Olivier Sigaud Class material \ud83d\udd17 Video","title":"Pitfalls in Policy Gradient methods"},{"location":"pg-pitfalls.html#pitfalls-in-policy-gradient-methods","text":"","title":"Pitfalls in Policy Gradient methods"},{"location":"pg-pitfalls.html#abstract","text":"In this talk, I will present the behavior of variants of the REINFORCE algorithm using simple gym classic control benchmarks (CartPole, Pendulum, MountainCar...) and various stochastic policy representations (Bernoulli, Gaussian, squashed Gaussian). I will highlight difficulties faced by these algorithms on those simple environments and draw lessons about the necessity to better understanding how they work or why they don't before moving to more advanced methods and more complex benchmarks.","title":"Abstract"},{"location":"pg-pitfalls.html#speaker","text":"Olivier Sigaud","title":"Speaker"},{"location":"pg-pitfalls.html#class-material","text":"Video","title":"Class material"},{"location":"pg.html","text":"From Policy Gradients to Actor Critic methods \ud83d\udd17 Abstract \ud83d\udd17 Starting from the general policy search problem and direct policy search methods, I will give a didactical presentation of the Policy Gradient Theorem and explain some variants of the REINFORCE algorithm. From there, I will move step by step to presenting more advanced methods such as TRPO, PPO and Actor-Critic methods such as DDPG and SAC. Speaker \ud83d\udd17 Olivier Sigaud Outline \ud83d\udd17 Introduction: the 4 routes to deep RL The Policy Search problem The policy gradient derivation PG with baseline versus Actor-Critic Bias-variance trade-off On-policy versus Off-policy TRPO and ACKTR Proximal Policy Optimization (PPO) Deep Deterministic Policy Gradient (and TD3) Soft Actor Critic Policy Gradient and Reward Weighted Regression Wrap-up, Take Home Messages Class material \ud83d\udd17 Video recording Slides: Introduction: the 4 routes to deep RL The Policy Search problem The policy gradient derivation ( part 1/3 ) ( part 2/3 ) ( part 3/3 ) PG with baseline versus Actor-Critic Bias variance trade-off On-policy versus Off-policy TRPO and ACKTR Proximal Policy Optimization (PPO) Deep Deterministic Policy Gradient (and TD3) Soft Actor Critic Policy gradient and Reward Weighted Regression Wrap-up, Take Home Messages","title":"Policy Gradients and Actor Critic methods"},{"location":"pg.html#from-policy-gradients-to-actor-critic-methods","text":"","title":"From Policy Gradients to Actor Critic methods"},{"location":"pg.html#abstract","text":"Starting from the general policy search problem and direct policy search methods, I will give a didactical presentation of the Policy Gradient Theorem and explain some variants of the REINFORCE algorithm. From there, I will move step by step to presenting more advanced methods such as TRPO, PPO and Actor-Critic methods such as DDPG and SAC.","title":"Abstract"},{"location":"pg.html#speaker","text":"Olivier Sigaud","title":"Speaker"},{"location":"pg.html#outline","text":"Introduction: the 4 routes to deep RL The Policy Search problem The policy gradient derivation PG with baseline versus Actor-Critic Bias-variance trade-off On-policy versus Off-policy TRPO and ACKTR Proximal Policy Optimization (PPO) Deep Deterministic Policy Gradient (and TD3) Soft Actor Critic Policy Gradient and Reward Weighted Regression Wrap-up, Take Home Messages","title":"Outline"},{"location":"pg.html#class-material","text":"Video recording Slides: Introduction: the 4 routes to deep RL The Policy Search problem The policy gradient derivation ( part 1/3 ) ( part 2/3 ) ( part 3/3 ) PG with baseline versus Actor-Critic Bias variance trade-off On-policy versus Off-policy TRPO and ACKTR Proximal Policy Optimization (PPO) Deep Deterministic Policy Gradient (and TD3) Soft Actor Critic Policy gradient and Reward Weighted Regression Wrap-up, Take Home Messages","title":"Class material"},{"location":"regret-bound.html","text":"Regret bounds of model-based reinforcement learning \ud83d\udd17 Abstract \ud83d\udd17 We discuss some recent results on model-based methods for online reinforcement learning (RL) . The goal of online RL is to adaptively explore an unknown environment and learn to act with provable regret bounds. In particular, we focus on finite-horizon episodic RL where the unknown transition law belongs to a generic family of models. We propose a model based \u2018value-targeted regression\u2019 RL algorithm that is based on optimism principle: In each episode, the set of models that are `consistent' with the data collected is constructed. The criterion of consistency is based on the total squared error of that the model incurs on the task of predicting values as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, for arbitrary family of transition models, using the notion of the so-called Eluder dimension proposed by Russo & Van Roy (2014). Speakers \ud83d\udd17 Mengdi Wang Class material \ud83d\udd17 Slides Video","title":"Regret bounds of model-based reinforcement learning"},{"location":"regret-bound.html#regret-bounds-of-model-based-reinforcement-learning","text":"","title":"Regret bounds of model-based reinforcement learning"},{"location":"regret-bound.html#abstract","text":"We discuss some recent results on model-based methods for online reinforcement learning (RL) . The goal of online RL is to adaptively explore an unknown environment and learn to act with provable regret bounds. In particular, we focus on finite-horizon episodic RL where the unknown transition law belongs to a generic family of models. We propose a model based \u2018value-targeted regression\u2019 RL algorithm that is based on optimism principle: In each episode, the set of models that are `consistent' with the data collected is constructed. The criterion of consistency is based on the total squared error of that the model incurs on the task of predicting values as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, for arbitrary family of transition models, using the notion of the so-called Eluder dimension proposed by Russo & Van Roy (2014).","title":"Abstract"},{"location":"regret-bound.html#speakers","text":"Mengdi Wang","title":"Speakers"},{"location":"regret-bound.html#class-material","text":"Slides Video","title":"Class material"},{"location":"regularized-mdps.html","text":"Regularized MDPs \ud83d\udd17 Abstract \ud83d\udd17 Many recent efficient deep reinforcement learning algorithms make use of some sort of regularization. This tutorial will review these approaches through the lens of regularized approximate dynamic programming, which allows connecting the different algorithms and explains theoretically why regularization works. Speaker \ud83d\udd17 Matthieu Geist Class material \ud83d\udd17 Slides Video","title":"Regularized MDPs"},{"location":"regularized-mdps.html#regularized-mdps","text":"","title":"Regularized MDPs"},{"location":"regularized-mdps.html#abstract","text":"Many recent efficient deep reinforcement learning algorithms make use of some sort of regularization. This tutorial will review these approaches through the lens of regularized approximate dynamic programming, which allows connecting the different algorithms and explains theoretically why regularization works.","title":"Abstract"},{"location":"regularized-mdps.html#speaker","text":"Matthieu Geist","title":"Speaker"},{"location":"regularized-mdps.html#class-material","text":"Slides Video","title":"Class material"},{"location":"rl-fundamentals.html","text":"Reinforcement Learning fundamentals \ud83d\udd17 Abstract \ud83d\udd17 The overall goal of this session is to provide the RLVS participants with an overview of RL and an understanding of (some) key challenges. It should also introduce participants to issues that will be tackled in more details in the following classes. We will follow the course of a notebook together and we will alternate between a \"lecture\" mode and small illustrative exercices (with corrections) that the participants will have time to try themselves. We will introduce the modeling fundamentals of Makov Decision Processes. Then we will move our way up from toy examples and fundamental algorithms to key challenges in RL. In particular, we will focus on three key structuring issues for the RL practitioner: the exploration/exploitation tradeoff, value function approximation, and optimality search. Along the session we will link each topic to the corresponding classes in RLVS. Speaker \ud83d\udd17 E. Rachelson Class material \ud83d\udd17 Notebook ( colab ) ( Original repo ) Video recording","title":"RL fundamentals"},{"location":"rl-fundamentals.html#reinforcement-learning-fundamentals","text":"","title":"Reinforcement Learning fundamentals"},{"location":"rl-fundamentals.html#abstract","text":"The overall goal of this session is to provide the RLVS participants with an overview of RL and an understanding of (some) key challenges. It should also introduce participants to issues that will be tackled in more details in the following classes. We will follow the course of a notebook together and we will alternate between a \"lecture\" mode and small illustrative exercices (with corrections) that the participants will have time to try themselves. We will introduce the modeling fundamentals of Makov Decision Processes. Then we will move our way up from toy examples and fundamental algorithms to key challenges in RL. In particular, we will focus on three key structuring issues for the RL practitioner: the exploration/exploitation tradeoff, value function approximation, and optimality search. Along the session we will link each topic to the corresponding classes in RLVS.","title":"Abstract"},{"location":"rl-fundamentals.html#speaker","text":"E. Rachelson","title":"Speaker"},{"location":"rl-fundamentals.html#class-material","text":"Notebook ( colab ) ( Original repo ) Video recording","title":"Class material"},{"location":"rlvs-overview.html","text":"Overview of the 2021 Reinforcement Learning Virtual School \ud83d\udd17 Abstract \ud83d\udd17 This short introduction will present the organization of RLVS 2021 and the progression of ideas across days and sessions. Speaker \ud83d\udd17 E. Rachelson Material \ud83d\udd17 Video recording Presentation slides","title":"RLVS overview"},{"location":"rlvs-overview.html#overview-of-the-2021-reinforcement-learning-virtual-school","text":"","title":"Overview of the 2021 Reinforcement Learning Virtual School"},{"location":"rlvs-overview.html#abstract","text":"This short introduction will present the organization of RLVS 2021 and the progression of ideas across days and sessions.","title":"Abstract"},{"location":"rlvs-overview.html#speaker","text":"E. Rachelson","title":"Speaker"},{"location":"rlvs-overview.html#material","text":"Video recording Presentation slides","title":"Material"},{"location":"sebastian-risi.html","text":"Sebastian Risi \ud83d\udd17 Sebastian Risi is a Professor at the IT University of Copenhagen where he co-directs the Robotics, Evolution and Art Lab (REAL). He is currently the principal investigator of a Sapere Aude: DFF Starting Grant. He has won several international scientific awards, including multiple best paper awards, the Distinguished Young Investigator in Artificial Life 2018 award, a Google Faculty Research Award in 2019, and an Amazon Research Award in 2020. His aim is to make machines more adaptive and to allow them to learn from and work together with humans. His work has been published in major machine learning and AI conferences, such as AAAI, NeurIPS, Nature Machine Intelligence, and conferences on human-computer interactions (CHI). Recently, he co-founded modl.ai, a company that let game developers rapidly create and test their games through novel AI approaches. More information: http://sebastianrisi.com/","title":"S. Risi"},{"location":"sebastian-risi.html#sebastian-risi","text":"Sebastian Risi is a Professor at the IT University of Copenhagen where he co-directs the Robotics, Evolution and Art Lab (REAL). He is currently the principal investigator of a Sapere Aude: DFF Starting Grant. He has won several international scientific awards, including multiple best paper awards, the Distinguished Young Investigator in Artificial Life 2018 award, a Google Faculty Research Award in 2019, and an Amazon Research Award in 2020. His aim is to make machines more adaptive and to allow them to learn from and work together with humans. His work has been published in major machine learning and AI conferences, such as AAAI, NeurIPS, Nature Machine Intelligence, and conferences on human-computer interactions (CHI). Recently, he co-founded modl.ai, a company that let game developers rapidly create and test their games through novel AI approaches. More information: http://sebastianrisi.com/","title":"Sebastian Risi"},{"location":"sebastien-gerchinovitz.html","text":"Sebastien Gerchinovitz \ud83d\udd17 I am a research scientist at IRT Saint Exup\u00e9ry, Toulouse, France, working in the DEEL project on machine learning theory and applications to safety-critical systems. I am also an associate researcher at the Institut de Math\u00e9matiques de Toulouse, and a member of the Game Theory and Artificial Intelligence chair within the Artificial and Natural Intelligence Toulouse Institute (ANITI). Currently on leave from Universit\u00e9 Toulouse III - Paul Sabatier where I previously held an assistant professor position. I received a PhD in Mathematics at Ecole Normale Sup\u00e9rieure, Paris. My main research topics are online learning, statistical learning theory, and deep learning theory. I have been in the Program Committees of ALT 2018 & 2019, ITW 2020, and COLT for several years. Research visits at University of Milan, UC Berkeley, and CWI Amsterdam.","title":"Sebastien Gerchinovitz"},{"location":"sebastien-gerchinovitz.html#sebastien-gerchinovitz","text":"I am a research scientist at IRT Saint Exup\u00e9ry, Toulouse, France, working in the DEEL project on machine learning theory and applications to safety-critical systems. I am also an associate researcher at the Institut de Math\u00e9matiques de Toulouse, and a member of the Game Theory and Artificial Intelligence chair within the Artificial and Natural Intelligence Toulouse Institute (ANITI). Currently on leave from Universit\u00e9 Toulouse III - Paul Sabatier where I previously held an assistant professor position. I received a PhD in Mathematics at Ecole Normale Sup\u00e9rieure, Paris. My main research topics are online learning, statistical learning theory, and deep learning theory. I have been in the Program Committees of ALT 2018 & 2019, ITW 2020, and COLT for several years. Research visits at University of Milan, UC Berkeley, and CWI Amsterdam.","title":"Sebastien Gerchinovitz"},{"location":"stochastic-bandits.html","text":"Stochastic bandits \ud83d\udd17 Abstract \ud83d\udd17 The bandit framework specialises the reinforcement learning setup by removing the (controlled) state. Bandits provide all the essential ingredients to study the exploration/exploitation dilemma, with many principles derived for bandits generalising to the reinforcement learning setting. The simplification has the advantage that it permits a more complete understanding and practical algorithms. Furthermore, bandits are a good model for many applications. I will introduce bandit problems and present the most well known algorithms based on the principle of optimism in the face of uncertainty. There will be a live coding demo and discussions of the many extensions needed in practical applications. Speaker \ud83d\udd17 Tor Lattimore Class material \ud83d\udd17 Slides Code The Bandit Algorithms book Notebook on colab implementing the same code as above. Video recording","title":"Stochastic bandits"},{"location":"stochastic-bandits.html#stochastic-bandits","text":"","title":"Stochastic bandits"},{"location":"stochastic-bandits.html#abstract","text":"The bandit framework specialises the reinforcement learning setup by removing the (controlled) state. Bandits provide all the essential ingredients to study the exploration/exploitation dilemma, with many principles derived for bandits generalising to the reinforcement learning setting. The simplification has the advantage that it permits a more complete understanding and practical algorithms. Furthermore, bandits are a good model for many applications. I will introduce bandit problems and present the most well known algorithms based on the principle of optimism in the face of uncertainty. There will be a live coding demo and discussions of the many extensions needed in practical applications.","title":"Abstract"},{"location":"stochastic-bandits.html#speaker","text":"Tor Lattimore","title":"Speaker"},{"location":"stochastic-bandits.html#class-material","text":"Slides Code The Bandit Algorithms book Notebook on colab implementing the same code as above. Video recording","title":"Class material"},{"location":"symbolic.html","text":"Symbolic representations and reinforcement learning \ud83d\udd17 Abstract \ud83d\udd17 A remarkable property of deep learning algorithms is their ability to learn useful task-specific representations from data directly without the need for hand-crafted feature engineering. As they have grown in popularity over the past decade deep neural networks (NNs) have been successfully applied to a wide range of machine learning tasks, achieving state of the art results across many research areas. However, as the complexity of the research problems increase some of the limitations of NN become increasingly clear: NNs suffer from interpretability issues, poor generalisation that leads to very data-hungry algorithms and the inability to be combined with other old, well established AI algorithms. Some of the research tackling these drawbacks takes inspiration from symbolic AI. It focusses, for example, on obtaining interpretable representations from NNs or thinking about objects and relations when building network architectures. This talk reviews symbolic approaches and properties that might be interesting to keep in the back of our heads for current representation learning and reviews current research that merges deep and symbolic methods with an emphasis on methods applied to reinforcement learning. Speaker \ud83d\udd17 Marta Garnelo Class material \ud83d\udd17","title":"Symbolic representations and reinforcement learning"},{"location":"symbolic.html#symbolic-representations-and-reinforcement-learning","text":"","title":"Symbolic representations and reinforcement learning"},{"location":"symbolic.html#abstract","text":"A remarkable property of deep learning algorithms is their ability to learn useful task-specific representations from data directly without the need for hand-crafted feature engineering. As they have grown in popularity over the past decade deep neural networks (NNs) have been successfully applied to a wide range of machine learning tasks, achieving state of the art results across many research areas. However, as the complexity of the research problems increase some of the limitations of NN become increasingly clear: NNs suffer from interpretability issues, poor generalisation that leads to very data-hungry algorithms and the inability to be combined with other old, well established AI algorithms. Some of the research tackling these drawbacks takes inspiration from symbolic AI. It focusses, for example, on obtaining interpretable representations from NNs or thinking about objects and relations when building network architectures. This talk reviews symbolic approaches and properties that might be interesting to keep in the back of our heads for current representation learning and reviews current research that merges deep and symbolic methods with an emphasis on methods applied to reinforcement learning.","title":"Abstract"},{"location":"symbolic.html#speaker","text":"Marta Garnelo","title":"Speaker"},{"location":"symbolic.html#class-material","text":"","title":"Class material"},{"location":"tips-and-tricks.html","text":"RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3 \ud83d\udd17 Abstract \ud83d\udd17 The aim of the session is to help you do reinforcement learning experiments. The first part covers general advice about RL, tips and tricks and details three examples where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library. Speaker \ud83d\udd17 Antonin Raffin Pre-requisites \ud83d\udd17 Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab). Additional material \ud83d\udd17 Stable Baselines 3 website Stable Baselines 3 documentation Presentation slides Hands-on session: presentation , repo and notebook on colab Outline \ud83d\udd17 Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots Introduction (3 minutes) RL Tips and tricks (45 minutes) General Nuts and Bolts of RL experimentation (10 minutes) RL in practice on a custom task (custom environment) (30 minutes) Questions? (5 minutes) The Challenges of Applying RL to Real Robots (45 minutes) Learning to control an elastic robot - DLR David Neck Example (15 minutes) Learning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes) Learning to walk with an elastic quadruped robot - DLR bert example (10 minutes) Questions? (5 minutes+) Part II: Practical Session with Stable-Baselines3 Stable-Baselines3 Overview (20 minutes) Questions? (5 minutes) Practical Session - Code along (1h+)","title":"RL tips and tricks"},{"location":"tips-and-tricks.html#rl-in-practice-tips-and-tricks-and-practical-session-with-stable-baselines3","text":"","title":"RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3"},{"location":"tips-and-tricks.html#abstract","text":"The aim of the session is to help you do reinforcement learning experiments. The first part covers general advice about RL, tips and tricks and details three examples where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library.","title":"Abstract"},{"location":"tips-and-tricks.html#speaker","text":"Antonin Raffin","title":"Speaker"},{"location":"tips-and-tricks.html#pre-requisites","text":"Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab).","title":"Pre-requisites"},{"location":"tips-and-tricks.html#additional-material","text":"Stable Baselines 3 website Stable Baselines 3 documentation Presentation slides Hands-on session: presentation , repo and notebook on colab","title":"Additional material"},{"location":"tips-and-tricks.html#outline","text":"Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots Introduction (3 minutes) RL Tips and tricks (45 minutes) General Nuts and Bolts of RL experimentation (10 minutes) RL in practice on a custom task (custom environment) (30 minutes) Questions? (5 minutes) The Challenges of Applying RL to Real Robots (45 minutes) Learning to control an elastic robot - DLR David Neck Example (15 minutes) Learning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes) Learning to walk with an elastic quadruped robot - DLR bert example (10 minutes) Questions? (5 minutes+) Part II: Practical Session with Stable-Baselines3 Stable-Baselines3 Overview (20 minutes) Questions? (5 minutes) Practical Session - Code along (1h+)","title":"Outline"},{"location":"tommaso-cesari.html","text":"Tommaso Cesari \ud83d\udd17 Tom Cesari is a Postdoc with ANITI and also affiliated with the Toulouse School of Economics","title":"Tommaso Cesari"},{"location":"tommaso-cesari.html#tommaso-cesari","text":"Tom Cesari is a Postdoc with ANITI and also affiliated with the Toulouse School of Economics","title":"Tommaso Cesari"},{"location":"tor-lattimore.html","text":"Tor Lattimore \ud83d\udd17 I am a research scientist at DeepMind working mostly on decision making algorithms. I was previously an assistant professor at Indiana University, Bloomington and before that a postdoc at the University of Alberta. I received my PhD from the Australian National University under the supervision of Marcus Hutter. Together with Csaba Szepesvari I am the author of the book \"Bandit Algorithms\", which is published by Cambridge University Press and freely available at http://banditalgs.com . https://tor-lattimore.com/","title":"T. Lattimore"},{"location":"tor-lattimore.html#tor-lattimore","text":"I am a research scientist at DeepMind working mostly on decision making algorithms. I was previously an assistant professor at Indiana University, Bloomington and before that a postdoc at the University of Alberta. I received my PhD from the Australian National University under the supervision of Marcus Hutter. Together with Csaba Szepesvari I am the author of the book \"Bandit Algorithms\", which is published by Cambridge University Press and freely available at http://banditalgs.com . https://tor-lattimore.com/","title":"Tor Lattimore"},{"location":"wrap-up.html","text":"RLVS wrap-up \ud83d\udd17 Abstract \ud83d\udd17 The final words on the scientific program, the next steps, the acknowledgements, and the warm goodbyes. Speakers \ud83d\udd17 E. Rachelson S. Gerchinovitz Material \ud83d\udd17 Presentation slides","title":"Wrap-up"},{"location":"wrap-up.html#rlvs-wrap-up","text":"","title":"RLVS wrap-up"},{"location":"wrap-up.html#abstract","text":"The final words on the scientific program, the next steps, the acknowledgements, and the warm goodbyes.","title":"Abstract"},{"location":"wrap-up.html#speakers","text":"E. Rachelson S. Gerchinovitz","title":"Speakers"},{"location":"wrap-up.html#material","text":"Presentation slides","title":"Material"},{"location":"class-material/evolutionary/map_elites_tutorial/index.html","text":"Notebook for the MAP-Elites tutorial \ud83d\udd17 Jupyter notebook for the MAP-Elites algorithms (Mouret & Clune, 2015) Reference: @article { mouret2015illuminating , title = {Illuminating search spaces by mapping elites} , author = {Mouret, Jean-Baptiste and Clune, Jeff} , journal = {arXiv preprint arXiv:1504.04909} , year = {2015} } Please note that this tutorial is designed to understand the basic concepts of MAP-Elites. If you want to use a simple (but more effective) implementation, you should use: - Python: https://github.resibots.com/pymap_elites - C++: https://github.com/sferes2","title":"Notebook for the MAP-Elites tutorial"},{"location":"class-material/evolutionary/map_elites_tutorial/index.html#notebook-for-the-map-elites-tutorial","text":"Jupyter notebook for the MAP-Elites algorithms (Mouret & Clune, 2015) Reference: @article { mouret2015illuminating , title = {Illuminating search spaces by mapping elites} , author = {Mouret, Jean-Baptiste and Clune, Jeff} , journal = {arXiv preprint arXiv:1504.04909} , year = {2015} } Please note that this tutorial is designed to understand the basic concepts of MAP-Elites. If you want to use a simple (but more effective) implementation, you should use: - Python: https://github.resibots.com/pymap_elites - C++: https://github.com/sferes2","title":"Notebook for the MAP-Elites tutorial"},{"location":"class-material/pg/slides.html","text":"Policy Gradients and Actor Critic methods \ud83d\udd17 1/ Introduction: the 4 routes to deep RL 2/ The Policy Search problem 3/ The policy gradient derivation (1/3) 4/ The policy gradient derivation (2/3) 5/ The policy gradient derivation (3/3) 6/ PG with baseline versus Actor-Critic 7/ Bias variance trade-off 8/ On-policy versus Off-policy 9/ TRPO and ACKTR 10/ Proximal Policy Optimization (PPO) 11/ Deep Deterministic Policy Gradient (and TD3) 12/ Soft Actor Critic 13/ Policy gradient and Reward Weighted Regression 14/ Wrap-up, Take Home Messages","title":"Policy Gradients and Actor Critic methods"},{"location":"class-material/pg/slides.html#policy-gradients-and-actor-critic-methods","text":"1/ Introduction: the 4 routes to deep RL 2/ The Policy Search problem 3/ The policy gradient derivation (1/3) 4/ The policy gradient derivation (2/3) 5/ The policy gradient derivation (3/3) 6/ PG with baseline versus Actor-Critic 7/ Bias variance trade-off 8/ On-policy versus Off-policy 9/ TRPO and ACKTR 10/ Proximal Policy Optimization (PPO) 11/ Deep Deterministic Policy Gradient (and TD3) 12/ Soft Actor Critic 13/ Policy gradient and Reward Weighted Regression 14/ Wrap-up, Take Home Messages","title":"Policy Gradients and Actor Critic methods"},{"location":"class-material/rl_fundamentals/index.html","text":"Reinforcement Learning fundamentals tutorial for RLVS 2021 \ud83d\udd17","title":"Reinforcement Learning fundamentals tutorial for RLVS 2021"},{"location":"class-material/rl_fundamentals/index.html#reinforcement-learning-fundamentals-tutorial-for-rlvs-2021","text":"","title":"Reinforcement Learning fundamentals tutorial for RLVS 2021"}]}