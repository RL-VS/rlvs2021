{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"RLVS 2021 \ud83d\udd17 Welcome to the 2021 Reinforcement Learning Virtual School's website. This edition is hosted by ANITI . Event frontpage: https://rlvs.aniti.fr . Event website (during the event): https://whova.com/portal/webapp/rlstc_202011/ . Mobile app: https://whova.com/portal/rlstc_202011 . Goals of RLVS \ud83d\udd17 RLVS intends to provide a high-quality, easy access to the field of Reinforcement Learning to new researchers. It aims to: - Allow attendees to step in to the field of RL - Learn from top lecturers in the field - Provide broad access to live classes We expect the participants to be a mixture of masters and Ph.D. students, academics, and industrial researchers with a solid background in mathematics and computer science. RLVS schedule \ud83d\udd17 This condensed schedule does not include class breaks and social events. Times are Central European Time (GMT+1). Schedule March 25th 9:00-9:30 RLVS Overview E. Rachelson 9:30-12:30 RL fundamentals E. Rachelson 14:00-16:00 Deep Learning D. Wilson 16:30-17:30 Human behavioral agents I. Rish March 26th 10:00-12:00 Stochastic bandits T. Lattimore 14:00-16:00 Monte Carlo Tree Search T. Lattimore 16:30-17:30 Multi-armed bandits in clinical trials D. A. Berry April 1st 9:00-15:00 Deep Q-Networks and its variants B. Piot 15:15-16:15 Regularized MDPs M. Geist 16:30-17:30 TBA M. Wang April 2nd 9:00-12:30 Policy Gradients and Actor Critic methods O. Sigaud 14:00-15:00 Pitfalls in Policy Gradient methods O. Sigaud 15:30-17:30 Exploration in Deep RL M. Pirotta April 8th 9:00-11:00 Evolutionary Reinforcement Learning D. Wilson , J.-B. Mouret 11:30-12:30 TBA S. Risi 14:00-16:00 Micro-data Policy Search K. Chatzilygeroudis , J.-B. Mouret 16:30-17:30 TBA April 9th 9:00-13:00 RL tips and tricks A. Raffin 14:30-15:30 TBA M. Garnelo 15:45-16:45 TBA L. P. Kaelbling 17:00-18:00 RLVS wrap-up E. Rachelson Speakers \ud83d\udd17 Sorted alphabetically Donald A. Berry University of Texas and Rice University Konstantinos Chatzilygeroudis University of Patras Marta Garnelo DeepMind & Imperial College Matthieu Geist Google Brain Leslie P. Kaelbling MIT Tor Lattimore DeepMind Jean-Baptiste Mouret INRIA Bilal Piot DeepMind Matteo Pirotta Facebook AI Research Emmanuel Rachelson ISAE-SUPAERO, Universit\u00e9 de Toulouse Antonin Raffin DLR Irina Rish Universit\u00e9 de Montr\u00e9al Sebastian Risi University of Copenhagen O. Sigaud Sorbonne Universit\u00e9 Mengdi Wang Princeton University Dennis Wilson ISAE-SUPAERO, Universit\u00e9 de Toulouse Organizers \ud83d\udd17 Program committee David Bertoin IRT Saint Exup\u00e9ry Tommaso Cesari Toulouse School of Economics S\u00e9bastien Gerchinovitz IRT Saint Exup\u00e9ry and Institut de Math\u00e9matiques de Toulouse Nicolas Mansard LAAS-CNRS Emmanuel Rachelson ISAE-SUPAERO Local arrangement chair Corinne Joffre Universit\u00e9 de Toulouse Communication chair Alix Fauque de Jonqui\u00e8res Universit\u00e9 de Toulouse","title":"RLVS home"},{"location":"index.html#rlvs-2021","text":"Welcome to the 2021 Reinforcement Learning Virtual School's website. This edition is hosted by ANITI . Event frontpage: https://rlvs.aniti.fr . Event website (during the event): https://whova.com/portal/webapp/rlstc_202011/ . Mobile app: https://whova.com/portal/rlstc_202011 .","title":"RLVS 2021"},{"location":"index.html#goals-of-rlvs","text":"RLVS intends to provide a high-quality, easy access to the field of Reinforcement Learning to new researchers. It aims to: - Allow attendees to step in to the field of RL - Learn from top lecturers in the field - Provide broad access to live classes We expect the participants to be a mixture of masters and Ph.D. students, academics, and industrial researchers with a solid background in mathematics and computer science.","title":"Goals of RLVS"},{"location":"index.html#rlvs-schedule","text":"This condensed schedule does not include class breaks and social events. Times are Central European Time (GMT+1). Schedule March 25th 9:00-9:30 RLVS Overview E. Rachelson 9:30-12:30 RL fundamentals E. Rachelson 14:00-16:00 Deep Learning D. Wilson 16:30-17:30 Human behavioral agents I. Rish March 26th 10:00-12:00 Stochastic bandits T. Lattimore 14:00-16:00 Monte Carlo Tree Search T. Lattimore 16:30-17:30 Multi-armed bandits in clinical trials D. A. Berry April 1st 9:00-15:00 Deep Q-Networks and its variants B. Piot 15:15-16:15 Regularized MDPs M. Geist 16:30-17:30 TBA M. Wang April 2nd 9:00-12:30 Policy Gradients and Actor Critic methods O. Sigaud 14:00-15:00 Pitfalls in Policy Gradient methods O. Sigaud 15:30-17:30 Exploration in Deep RL M. Pirotta April 8th 9:00-11:00 Evolutionary Reinforcement Learning D. Wilson , J.-B. Mouret 11:30-12:30 TBA S. Risi 14:00-16:00 Micro-data Policy Search K. Chatzilygeroudis , J.-B. Mouret 16:30-17:30 TBA April 9th 9:00-13:00 RL tips and tricks A. Raffin 14:30-15:30 TBA M. Garnelo 15:45-16:45 TBA L. P. Kaelbling 17:00-18:00 RLVS wrap-up E. Rachelson","title":"RLVS schedule"},{"location":"index.html#speakers","text":"Sorted alphabetically Donald A. Berry University of Texas and Rice University Konstantinos Chatzilygeroudis University of Patras Marta Garnelo DeepMind & Imperial College Matthieu Geist Google Brain Leslie P. Kaelbling MIT Tor Lattimore DeepMind Jean-Baptiste Mouret INRIA Bilal Piot DeepMind Matteo Pirotta Facebook AI Research Emmanuel Rachelson ISAE-SUPAERO, Universit\u00e9 de Toulouse Antonin Raffin DLR Irina Rish Universit\u00e9 de Montr\u00e9al Sebastian Risi University of Copenhagen O. Sigaud Sorbonne Universit\u00e9 Mengdi Wang Princeton University Dennis Wilson ISAE-SUPAERO, Universit\u00e9 de Toulouse","title":"Speakers"},{"location":"index.html#organizers","text":"Program committee David Bertoin IRT Saint Exup\u00e9ry Tommaso Cesari Toulouse School of Economics S\u00e9bastien Gerchinovitz IRT Saint Exup\u00e9ry and Institut de Math\u00e9matiques de Toulouse Nicolas Mansard LAAS-CNRS Emmanuel Rachelson ISAE-SUPAERO Local arrangement chair Corinne Joffre Universit\u00e9 de Toulouse Communication chair Alix Fauque de Jonqui\u00e8res Universit\u00e9 de Toulouse","title":"Organizers"},{"location":"alix-fauque-de-jonquieres.html","text":"Alix Fauque de Jonqui\u00e8res \ud83d\udd17","title":"Alix Fauque de Jonqui\u00e8res"},{"location":"alix-fauque-de-jonquieres.html#alix-fauque-de-jonquieres","text":"","title":"Alix Fauque de Jonqui\u00e8res"},{"location":"antonin-raffin.html","text":"Antonin Raffin \ud83d\udd17 https://araffin.github.io/","title":"A. Raffin"},{"location":"antonin-raffin.html#antonin-raffin","text":"https://araffin.github.io/","title":"Antonin Raffin"},{"location":"bilal-piot.html","text":"Bilal Piot \ud83d\udd17","title":"B. Piot"},{"location":"bilal-piot.html#bilal-piot","text":"","title":"Bilal Piot"},{"location":"clinical.html","text":"Multi-armed bandits in clinical trials \ud83d\udd17","title":"Multi-armed bandits in clinical trials"},{"location":"clinical.html#multi-armed-bandits-in-clinical-trials","text":"","title":"Multi-armed bandits in clinical trials"},{"location":"corinne-joffre.html","text":"Corinne Joffre \ud83d\udd17","title":"Corinne Joffre"},{"location":"corinne-joffre.html#corinne-joffre","text":"","title":"Corinne Joffre"},{"location":"david-bertoin.html","text":"David Bertoin \ud83d\udd17","title":"David Bertoin"},{"location":"david-bertoin.html#david-bertoin","text":"","title":"David Bertoin"},{"location":"deep-learning.html","text":"Introduction to Deep Learning \ud83d\udd17","title":"Intro to Deep Learning"},{"location":"deep-learning.html#introduction-to-deep-learning","text":"","title":"Introduction to Deep Learning"},{"location":"dennis-wilson.html","text":"Dennis Wilson \ud83d\udd17 I am an assistant professor in AI and Data Science at ISAE-SUPAERO . My research focuses on bio-inspired artificial intelligence, namely genetic programming, neural networks, and the evolution of learning. I obtained my PhD at the Institut de Recherche en Informatique de Toulouse (IRIT) on the evolution of design principles for artificial neural networks. Prior to that, I worked in the Anyscale Learning For All group in CSAIL, MIT, applying evolutionary strategies and developmental models to the problem of wind farm layout optimization. I\u2019m currently co-organizer of the Supaero Reinforcement Learning Initiative ( SuReLi ), head of the Data & Decision Sciences Master\u2019s program, and am a part of the Decision Systems research group in the Department of Complex Systems Engineering.","title":"D. Wilson"},{"location":"dennis-wilson.html#dennis-wilson","text":"I am an assistant professor in AI and Data Science at ISAE-SUPAERO . My research focuses on bio-inspired artificial intelligence, namely genetic programming, neural networks, and the evolution of learning. I obtained my PhD at the Institut de Recherche en Informatique de Toulouse (IRIT) on the evolution of design principles for artificial neural networks. Prior to that, I worked in the Anyscale Learning For All group in CSAIL, MIT, applying evolutionary strategies and developmental models to the problem of wind farm layout optimization. I\u2019m currently co-organizer of the Supaero Reinforcement Learning Initiative ( SuReLi ), head of the Data & Decision Sciences Master\u2019s program, and am a part of the Decision Systems research group in the Department of Complex Systems Engineering.","title":"Dennis Wilson"},{"location":"donald-berry.html","text":"Donald A. Berry \ud83d\udd17 Donald Berry is Founder of Berry Consultants, LLC, and a professor in the Department of Biostatistics of the University of Texas M.D. Anderson Cancer Center. He was founding Chair of this department in 1999. Dr. Berry received his Ph.D. in statistics from Yale University, and previously served on the faculty at the University of Minnesota and at Duke University. He has held endowed faculty positions at Duke University and M.D. Anderson. Dr. Berry is a well known developer of statistical theory and methodology. He has designed and supervised the conduct of hundreds of innovative clinical trials, including Bayesian adaptive trials in cancer and other diseases. He has developed Bayesian adaptive designs that efficiently use information that accrues over the course of the trial. These trials minimize sample size while increasing the likelihood of detecting therapeutic activity. A principal focus of his research is the use of biomarkers for learning which patients benefit from which therapies, based on molecular markers and phenotype. In particular, he designed and is a co-PI of I-SPY 2 in high-risk early breast cancer, a trial that was the focus of the two lead articles with two editorials in the July 2016 NEJM. He has designed Phase 3 Bayesian adaptive platform clinical trials in glioblastoma, GBM-AGILE and pancreatic cancer. He has authored many books on statistics and biostatistics and has over 400 peer-reviewed publications. He is a Thomson Reuters Highly Cited Researcher in recognition of ranking among the top 1% of most cited researchers in Clinical Medicine. He is a Fellow of the American Statistical Association, the Institute of Mathematical Statistics, and the International Society of Bayesian Analysis. He has received numerous research grants from the U.S. NIH and NSF. https://faculty.mdanderson.org/profiles/donald_berry.html https://www.berryconsultants.com/don-berry/","title":"D. A. Berry"},{"location":"donald-berry.html#donald-a-berry","text":"Donald Berry is Founder of Berry Consultants, LLC, and a professor in the Department of Biostatistics of the University of Texas M.D. Anderson Cancer Center. He was founding Chair of this department in 1999. Dr. Berry received his Ph.D. in statistics from Yale University, and previously served on the faculty at the University of Minnesota and at Duke University. He has held endowed faculty positions at Duke University and M.D. Anderson. Dr. Berry is a well known developer of statistical theory and methodology. He has designed and supervised the conduct of hundreds of innovative clinical trials, including Bayesian adaptive trials in cancer and other diseases. He has developed Bayesian adaptive designs that efficiently use information that accrues over the course of the trial. These trials minimize sample size while increasing the likelihood of detecting therapeutic activity. A principal focus of his research is the use of biomarkers for learning which patients benefit from which therapies, based on molecular markers and phenotype. In particular, he designed and is a co-PI of I-SPY 2 in high-risk early breast cancer, a trial that was the focus of the two lead articles with two editorials in the July 2016 NEJM. He has designed Phase 3 Bayesian adaptive platform clinical trials in glioblastoma, GBM-AGILE and pancreatic cancer. He has authored many books on statistics and biostatistics and has over 400 peer-reviewed publications. He is a Thomson Reuters Highly Cited Researcher in recognition of ranking among the top 1% of most cited researchers in Clinical Medicine. He is a Fellow of the American Statistical Association, the Institute of Mathematical Statistics, and the International Society of Bayesian Analysis. He has received numerous research grants from the U.S. NIH and NSF. https://faculty.mdanderson.org/profiles/donald_berry.html https://www.berryconsultants.com/don-berry/","title":"Donald A. Berry"},{"location":"dqn.html","text":"Deep Q-Networks and its variants \ud83d\udd17 Abstract \ud83d\udd17 We will present in a coherent paradigm the different breakthroughs that led to the seminal paper Deep Q-Networks (DQN). Starting from dynamic programming and the value iteration algorithm, we will show how DQN can be seen as a particular instance of an approximate value iteration algorithm. Then, we will present an open-source codebase released by DeepMind called dqn_zoo that implements in JAX the DQN algorithm. Finally, we will present the variants of DQN, from Double DQN (DDQN) to Implicite Quantile DQN (IQN) and more. Prerequisites \ud83d\udd17 Understanding of Markov Decisions Processes and Bellman equations (starting point of the presentation) Basic notions of Python and JAX. (Optional) Run the dqn_zoo codebase.","title":"Deep Q-Networks and its variants"},{"location":"dqn.html#deep-q-networks-and-its-variants","text":"","title":"Deep Q-Networks and its variants"},{"location":"dqn.html#abstract","text":"We will present in a coherent paradigm the different breakthroughs that led to the seminal paper Deep Q-Networks (DQN). Starting from dynamic programming and the value iteration algorithm, we will show how DQN can be seen as a particular instance of an approximate value iteration algorithm. Then, we will present an open-source codebase released by DeepMind called dqn_zoo that implements in JAX the DQN algorithm. Finally, we will present the variants of DQN, from Double DQN (DDQN) to Implicite Quantile DQN (IQN) and more.","title":"Abstract"},{"location":"dqn.html#prerequisites","text":"Understanding of Markov Decisions Processes and Bellman equations (starting point of the presentation) Basic notions of Python and JAX. (Optional) Run the dqn_zoo codebase.","title":"Prerequisites"},{"location":"emmanuel-rachelson.html","text":"Emmanuel Rachelson \ud83d\udd17 I am a professor of Machine Learning and Optimization at ISAE-SUPAERO . My research lies in the fields of Reinforcement Learning and Sequential Decision problems, with a broader interest for Machine Learning and Operations Research. I am the part of the \"Decision Systems\" research group within the Department of Complex Systems Engineering (DISC) and the founder and co-head of SuReLI (Supaero Reinforcement Learning Initiative). I created the Data & Decision Sciences MS curriculum and am implied more generally on (most) AI-related topics within the cursus. My website: https://personnel.isae-supaero.fr/emmanuel-rachelson","title":"E. Rachelson"},{"location":"emmanuel-rachelson.html#emmanuel-rachelson","text":"I am a professor of Machine Learning and Optimization at ISAE-SUPAERO . My research lies in the fields of Reinforcement Learning and Sequential Decision problems, with a broader interest for Machine Learning and Operations Research. I am the part of the \"Decision Systems\" research group within the Department of Complex Systems Engineering (DISC) and the founder and co-head of SuReLI (Supaero Reinforcement Learning Initiative). I created the Data & Decision Sciences MS curriculum and am implied more generally on (most) AI-related topics within the cursus. My website: https://personnel.isae-supaero.fr/emmanuel-rachelson","title":"Emmanuel Rachelson"},{"location":"evo-rl.html","text":"Evolutionary reinforcement learning \ud83d\udd17 Abstract \ud83d\udd17 Reinforcement learning traditionally takes inspiration from operant conditioning, that is, trial-and-error learning during the lifetime of the agent. However, evolution is another trial-and-error process that is very successful in nature. This process inspired many algorithms that can also solve reinforcement learning problems while using a very different set of metaphors; in that case, learning happens at the phylogenetic timescale, from generation to generation. While evolutionary learning has its strengths, it also raises its own challenges. In this class, we will focus on the representation problem \u201cHow can an artificial genotype describe a neural network that could be as complex as a brain?\u201d\u2014and the stepping stones problem\u2014\u201cwhat intermediate steps lead to an artifact as sophisticated as a brain?\u201d. We will also draw parallels with traditional reinforcement learning methods and attempt to understand the strengths and weaknesses of each family of methods. Additional material \ud83d\udd17 Readings: [1] https://www.nature.com/articles/s42256-018-0006-z [2] https://www.cell.com/iscience/fulltext/S2589-0042(20)30928-7","title":"Evolutionary Reinforcement Learning"},{"location":"evo-rl.html#evolutionary-reinforcement-learning","text":"","title":"Evolutionary reinforcement learning"},{"location":"evo-rl.html#abstract","text":"Reinforcement learning traditionally takes inspiration from operant conditioning, that is, trial-and-error learning during the lifetime of the agent. However, evolution is another trial-and-error process that is very successful in nature. This process inspired many algorithms that can also solve reinforcement learning problems while using a very different set of metaphors; in that case, learning happens at the phylogenetic timescale, from generation to generation. While evolutionary learning has its strengths, it also raises its own challenges. In this class, we will focus on the representation problem \u201cHow can an artificial genotype describe a neural network that could be as complex as a brain?\u201d\u2014and the stepping stones problem\u2014\u201cwhat intermediate steps lead to an artifact as sophisticated as a brain?\u201d. We will also draw parallels with traditional reinforcement learning methods and attempt to understand the strengths and weaknesses of each family of methods.","title":"Abstract"},{"location":"evo-rl.html#additional-material","text":"Readings: [1] https://www.nature.com/articles/s42256-018-0006-z [2] https://www.cell.com/iscience/fulltext/S2589-0042(20)30928-7","title":"Additional material"},{"location":"exploration.html","text":"Exploration-Exploitation in Reinforcement Learning: Function Approximation and Deep RL \ud83d\udd17 One of the major challenges in reinforcement learning (RL) is the trade-off between exploration of the environment to gather information and exploitation of the samples observed so far to execute \"good\" (nearly optimal) actions. In this seminar, we review how exploration techniques are paired with function approximation in continuous state-action spaces. In particular, we will focus on the integration of exploration mechanisms with deep learning techniques. The seminar should provide enough theoretical and algorithmic background to understand existing techniques and possibly devise novel methods. Throughout the talk, we will discuss open problems and possible future research directions.","title":"Exploration in Deep RL"},{"location":"exploration.html#exploration-exploitation-in-reinforcement-learning-function-approximation-and-deep-rl","text":"One of the major challenges in reinforcement learning (RL) is the trade-off between exploration of the environment to gather information and exploitation of the samples observed so far to execute \"good\" (nearly optimal) actions. In this seminar, we review how exploration techniques are paired with function approximation in continuous state-action spaces. In particular, we will focus on the integration of exploration mechanisms with deep learning techniques. The seminar should provide enough theoretical and algorithmic background to understand existing techniques and possibly devise novel methods. Throughout the talk, we will discuss open problems and possible future research directions.","title":"Exploration-Exploitation in Reinforcement Learning: Function Approximation and Deep RL"},{"location":"human-behavioral-agents.html","text":"Human Behavioral Agents \ud83d\udd17","title":"Human behavioral agents"},{"location":"human-behavioral-agents.html#human-behavioral-agents","text":"","title":"Human Behavioral Agents"},{"location":"irina-rish.html","text":"Irina Rish \ud83d\udd17 Irina Rish is an Associate Professor in the Computer Science and Operations Research department at the Universit\u00e9 de Montr\u00e9al (UdeM) and a core member of Mila \u2013 Quebec AI Institute. She holds the Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI. She holds MSc and PhD in AI from University of California, Irvine and MSc in Applied Mathematics from Moscow Gubkin Institute. Dr. Rish\u2019s research focus is on machine learning, neural data analysis and neuroscience-inspired AI. Her current research interests include continual lifelong learning, optimization algorithms for deep neural networks, sparse modelling and probabilistic inference, dialog generation, biologically plausible reinforcement learning, and dynamical systems approaches to brain imaging analysis. Before joining UdeM and Mila in 2019, she was a research scientist at the IBM T. J. Watson Research Center, where she worked on various projects at the intersection of neuroscience and AI, and led the Neuro-AI challenge. Dr. Rish holds 64 patents, has published over 80 research papers, several book chapters, three edited books, and a monograph on Sparse Modelling. https://mila.quebec/en/person/irina-rish/ https://sites.google.com/site/irinarish/","title":"I. Rish"},{"location":"irina-rish.html#irina-rish","text":"Irina Rish is an Associate Professor in the Computer Science and Operations Research department at the Universit\u00e9 de Montr\u00e9al (UdeM) and a core member of Mila \u2013 Quebec AI Institute. She holds the Canada CIFAR AI Chair and the Canadian Excellence Research Chair in Autonomous AI. She holds MSc and PhD in AI from University of California, Irvine and MSc in Applied Mathematics from Moscow Gubkin Institute. Dr. Rish\u2019s research focus is on machine learning, neural data analysis and neuroscience-inspired AI. Her current research interests include continual lifelong learning, optimization algorithms for deep neural networks, sparse modelling and probabilistic inference, dialog generation, biologically plausible reinforcement learning, and dynamical systems approaches to brain imaging analysis. Before joining UdeM and Mila in 2019, she was a research scientist at the IBM T. J. Watson Research Center, where she worked on various projects at the intersection of neuroscience and AI, and led the Neuro-AI challenge. Dr. Rish holds 64 patents, has published over 80 research papers, several book chapters, three edited books, and a monograph on Sparse Modelling. https://mila.quebec/en/person/irina-rish/ https://sites.google.com/site/irinarish/","title":"Irina Rish"},{"location":"jean-baptiste-mouret.html","text":"Jean-Baptiste Mouret \ud83d\udd17 I study machine learning and evolutionary computation as a means to design highly adaptive robots. https://members.loria.fr/JBMouret/index.html","title":"J.-B. Mouret"},{"location":"jean-baptiste-mouret.html#jean-baptiste-mouret","text":"I study machine learning and evolutionary computation as a means to design highly adaptive robots. https://members.loria.fr/JBMouret/index.html","title":"Jean-Baptiste Mouret"},{"location":"konstantinos-chatzilygeroudis.html","text":"Konstantinos Chatzilygeroudis \ud83d\udd17 I am currently a Post-doctoral fellow in Machine Learning and Robotics at the Computational Intelligence Laboratory , Department of Mathematics, University of Patras, in collaboration with Prof. Michael N. Vrahatis . My work focuses on using evolutionary and machine learning methods combined with simulations to develop trial-and-error methods that work on physical robots. I am also the Leader of the R&D Computer Vision Team at Metargus , a pre-seed funded start up (based in Patras, Greece), building a cutting-edge basketball coaching tool to provide coaches with insights far beyond traditional analytics. My work focuses on combining traditional Computer Vision techniques with modern Machine/Deep Learning algorithms. I am also currently teaching the under-graduate courses \"Introduction to Artificial Intelligence\" and \"Robotics and Intelligent Agents\" at the Computer Engineering & Informatics Department of University of Patras. http://costashatz.github.io/","title":"K. Chatzilygeroudis"},{"location":"konstantinos-chatzilygeroudis.html#konstantinos-chatzilygeroudis","text":"I am currently a Post-doctoral fellow in Machine Learning and Robotics at the Computational Intelligence Laboratory , Department of Mathematics, University of Patras, in collaboration with Prof. Michael N. Vrahatis . My work focuses on using evolutionary and machine learning methods combined with simulations to develop trial-and-error methods that work on physical robots. I am also the Leader of the R&D Computer Vision Team at Metargus , a pre-seed funded start up (based in Patras, Greece), building a cutting-edge basketball coaching tool to provide coaches with insights far beyond traditional analytics. My work focuses on combining traditional Computer Vision techniques with modern Machine/Deep Learning algorithms. I am also currently teaching the under-graduate courses \"Introduction to Artificial Intelligence\" and \"Robotics and Intelligent Agents\" at the Computer Engineering & Informatics Department of University of Patras. http://costashatz.github.io/","title":"Konstantinos Chatzilygeroudis"},{"location":"leslie-kaelbling.html","text":"Leslie Pack Kaelbling \ud83d\udd17 Leslie Pack Kaelbling is Professor of Computer Science and Engineering at MIT. She has previously held positions at Brown University, the Artificial Intelligence Center of SRI International, and at Teleos Research. Prof. Kaelbling has done substantial research on designing situated agents, mobile robotics, reinforcement learning, and decision-theoretic planning. In 2000, she founded the Journal of Machine Learning Research, a high-quality journal that is both freely available electronically as well as published in archival form; she currently serves as editor-in-chief. She is an NSF Presidential Faculty Fellow, a former member of the AAAI Executive Council, the 1997 recipient of the IJCAI Computers and Thought Award, a trustee of IJCAII and a fellow of the AAAI. She received an A. B. in Philosophy in 1983 and a Ph. D. in Computer Science in 1990, both from Stanford University. https://www.csail.mit.edu/person/leslie-kaelbling","title":"L. P. Kaelbling"},{"location":"leslie-kaelbling.html#leslie-pack-kaelbling","text":"Leslie Pack Kaelbling is Professor of Computer Science and Engineering at MIT. She has previously held positions at Brown University, the Artificial Intelligence Center of SRI International, and at Teleos Research. Prof. Kaelbling has done substantial research on designing situated agents, mobile robotics, reinforcement learning, and decision-theoretic planning. In 2000, she founded the Journal of Machine Learning Research, a high-quality journal that is both freely available electronically as well as published in archival form; she currently serves as editor-in-chief. She is an NSF Presidential Faculty Fellow, a former member of the AAAI Executive Council, the 1997 recipient of the IJCAI Computers and Thought Award, a trustee of IJCAII and a fellow of the AAAI. She received an A. B. in Philosophy in 1983 and a Ph. D. in Computer Science in 1990, both from Stanford University. https://www.csail.mit.edu/person/leslie-kaelbling","title":"Leslie Pack Kaelbling"},{"location":"marta-garnelo.html","text":"Marta Garnelo \ud83d\udd17","title":"M. Garnelo"},{"location":"marta-garnelo.html#marta-garnelo","text":"","title":"Marta Garnelo"},{"location":"matteo-pirotta.html","text":"Matteo Pirotta \ud83d\udd17 I am research scientist at Facebook AI Research in Paris. Previously, I was postdoc at INRIA Lille - Nord Europe in the SequeL team for almost two years. Before I was postdoc at Politecnico di Milano . I have received my PhD in computer science at Politecnico di Milano, under the supervision of Luca Bascetta and Marcello Restelli . My research interest is machine learning. In particular I am interested in reinforcement learning, transfer learning and online learning. https://teopir.github.io/","title":"M. Pirotta"},{"location":"matteo-pirotta.html#matteo-pirotta","text":"I am research scientist at Facebook AI Research in Paris. Previously, I was postdoc at INRIA Lille - Nord Europe in the SequeL team for almost two years. Before I was postdoc at Politecnico di Milano . I have received my PhD in computer science at Politecnico di Milano, under the supervision of Luca Bascetta and Marcello Restelli . My research interest is machine learning. In particular I am interested in reinforcement learning, transfer learning and online learning. https://teopir.github.io/","title":"Matteo Pirotta"},{"location":"matthieu-geist.html","text":"Matthieu Geist \ud83d\udd17 Matthieu Geist obtained an Electrical Engineering degree and an MSc degree in Applied Mathematics in Sept. 2006 (Sup\u00e9lec, France), a PhD degree in Applied Mathematics in Nov. 2009 (University Paul Verlaine of Metz, France) and a Habilitation degree in Feb. 2016 (University Lille 1, France). Between Feb. 2010 and Sept. 2017, he was an assistant professor at CentraleSup\u00e9lec, France. In Sept. 2017, he joined University of Lorraine, France, as a full professor in Applied Mathematics (Interdisciplinary Laboratory for Continental Environments, CNRS-UL). Since Sept. 2018, he is on secondment at Google Brain, as a research scientist (Paris, France). His research interests include machine learning, especially reinforcement learning and imitation learning. https://research.google/people/106211/ Google scholar","title":"M. Geist"},{"location":"matthieu-geist.html#matthieu-geist","text":"Matthieu Geist obtained an Electrical Engineering degree and an MSc degree in Applied Mathematics in Sept. 2006 (Sup\u00e9lec, France), a PhD degree in Applied Mathematics in Nov. 2009 (University Paul Verlaine of Metz, France) and a Habilitation degree in Feb. 2016 (University Lille 1, France). Between Feb. 2010 and Sept. 2017, he was an assistant professor at CentraleSup\u00e9lec, France. In Sept. 2017, he joined University of Lorraine, France, as a full professor in Applied Mathematics (Interdisciplinary Laboratory for Continental Environments, CNRS-UL). Since Sept. 2018, he is on secondment at Google Brain, as a research scientist (Paris, France). His research interests include machine learning, especially reinforcement learning and imitation learning. https://research.google/people/106211/ Google scholar","title":"Matthieu Geist"},{"location":"mcts.html","text":"Monte Carlo Tree Search \ud83d\udd17","title":"Monte Carlo Tree Search"},{"location":"mcts.html#monte-carlo-tree-search","text":"","title":"Monte Carlo Tree Search"},{"location":"mengdi-wang.html","text":"Mendgi Wang \ud83d\udd17 Mengdi Wang is an associate professor at the Department of Electrical Engineering and Center for Statistics and Machine Learning at Princeton University. She is also affiliated with the Department of Computer Science and a visiting research scientist at DeepMind. Her research focuses on data-driven stochastic optimization and applications in machine and reinforcement learning. She received her PhD in Electrical Engineering and Computer Science from Massachusetts Institute of Technology in 2013. At MIT, Mengdi was affiliated with the Laboratory for Information and Decision Systems and was advised by Dimitri P. Bertsekas. Mengdi received the Young Researcher Prize in Continuous Optimization of the Mathematical Optimization Society in 2016 (awarded once every three years), the Princeton SEAS Innovation Award in 2016, the NSF Career Award in 2017, the Google Faculty Award in 2017, and the MIT Tech Review 35-Under-35 Innovation Award (China region) in 2018. She serves as an associate editor for Operations Research and Mathematics of Operations Research, as area chair for ICML, NeurIPS, AISTATS, and is on the editorial board of Journal of Machine Learning Research. Research supported by NSF, AFOSR, NIH, ONR, Google, Microsoft C3.ai DTI, FinUP. https://mwang.princeton.edu/","title":"M. Wang"},{"location":"mengdi-wang.html#mendgi-wang","text":"Mengdi Wang is an associate professor at the Department of Electrical Engineering and Center for Statistics and Machine Learning at Princeton University. She is also affiliated with the Department of Computer Science and a visiting research scientist at DeepMind. Her research focuses on data-driven stochastic optimization and applications in machine and reinforcement learning. She received her PhD in Electrical Engineering and Computer Science from Massachusetts Institute of Technology in 2013. At MIT, Mengdi was affiliated with the Laboratory for Information and Decision Systems and was advised by Dimitri P. Bertsekas. Mengdi received the Young Researcher Prize in Continuous Optimization of the Mathematical Optimization Society in 2016 (awarded once every three years), the Princeton SEAS Innovation Award in 2016, the NSF Career Award in 2017, the Google Faculty Award in 2017, and the MIT Tech Review 35-Under-35 Innovation Award (China region) in 2018. She serves as an associate editor for Operations Research and Mathematics of Operations Research, as area chair for ICML, NeurIPS, AISTATS, and is on the editorial board of Journal of Machine Learning Research. Research supported by NSF, AFOSR, NIH, ONR, Google, Microsoft C3.ai DTI, FinUP. https://mwang.princeton.edu/","title":"Mendgi Wang"},{"location":"micro-data.html","text":"Micro-data policy search \ud83d\udd17 Abstract \ud83d\udd17 Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible when experiments takes time or are expensive (for instance, with physical robot or with an aerodynamics simulator). This class focuses on the extreme other end of the spectrum: how can an algorithm adapt a policy with only a handful of trials (a dozen) and a few minutes? By analogy with the word \"big-data\", we refer to this challenge as \"micro-data reinforcement learning\". We will describe two main strategies: (1) leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators), and (2) create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Most of the examples will be about robotic systems, but the principle apply to any other expensive setup. Additional material \ud83d\udd17 Readings: [1] https://arxiv.org/abs/1807.02303","title":"Micro-data Policy Search"},{"location":"micro-data.html#micro-data-policy-search","text":"","title":"Micro-data policy search"},{"location":"micro-data.html#abstract","text":"Most policy search algorithms require thousands of training episodes to find an effective policy, which is often infeasible when experiments takes time or are expensive (for instance, with physical robot or with an aerodynamics simulator). This class focuses on the extreme other end of the spectrum: how can an algorithm adapt a policy with only a handful of trials (a dozen) and a few minutes? By analogy with the word \"big-data\", we refer to this challenge as \"micro-data reinforcement learning\". We will describe two main strategies: (1) leverage prior knowledge on the policy structure (e.g., dynamic movement primitives), on the policy parameters (e.g., demonstrations), or on the dynamics (e.g., simulators), and (2) create data-driven surrogate models of the expected reward (e.g., Bayesian optimization) or the dynamical model (e.g., model-based policy search), so that the policy optimizer queries the model instead of the real system. Most of the examples will be about robotic systems, but the principle apply to any other expensive setup.","title":"Abstract"},{"location":"micro-data.html#additional-material","text":"Readings: [1] https://arxiv.org/abs/1807.02303","title":"Additional material"},{"location":"nicolas-mansard.html","text":"Nicolas Mansard \ud83d\udd17","title":"Nicolas Mansard"},{"location":"nicolas-mansard.html#nicolas-mansard","text":"","title":"Nicolas Mansard"},{"location":"olivier-sigaud.html","text":"Olivier Sigaud \ud83d\udd17 Biography \ud83d\udd17 (1996) PhD in Computer Science \"Learning : from control to behavior\", advisor: Dominique Luzeaux (DGA) (1995-2001) Research engineer, Departement for Advanced Studies, at DASSAULT AVIATION (Aerospace company). (2001-2005) Lecturer in Computer Science UPMC-Paris6 (LIP6, AnimatLab) (2002) PhD in Philosophy \" Automatisme et subjectivit\u00e9 : l'anticipation au coeur de l'exp\u00e9rience\", advisor: Jacques Dubucs (IHSPST) (2004) HDR in Computer Science \"Adaptive Behavior for Agents in Complex Software Settings\" (in french), UPMC-Paris 6 (2005-2006) Professor in Computer Science UPMC-Paris6 (LIP6, AnimatLab) (2007- ) Professor in Computer Science UPMC-Paris6 (ISIR, AMAC team) Research Activities \ud83d\udd17 Understanding intelligence is a fascinating research topic. For a long time, artificial intelligence researchers have focused on typically human intellectual activities such as playing chess, having a conversation or proving theorems. But it is more and more obvious that these specific capabilities are anchored into apparently simpler capabilities such as using objects or tools, displaying social signals through the body posture or more simply moving one's body to interact appropriately with the environment. Performing robotics research reveals the difficulty of these problems that we solve effortlessly. Rather than addressing them through the standard engineering approach, it seems appropriate to try to figure out how our brain solves them. This amounts to investigating the psychological mechanisms through which kids acquire their motor and cognitive capabilities and neurophysiological mechanisms the brain uses to implement this learning process. In this context, my research is focused on designing and exploiting machine learning techniques dedicated to growing motor and cognitive capabilities in robots, and to modelling these capabilities in living beings. In practice, my work belongs to three domains: machine learning, where I'm interested in regression, reinforcement learning, stochastic optimisation and the combination of these methods. This domain provides the fundamental tools for the other two activities; developmental robotics, which strive to endow robots with psychological mechanisms similar to those of kids; computational neurosciences, which strive to design computational models of the neurophysiological mechanism of animal learning. Within ISIR, I am responsible of a group about Learning for Control and Decision in Robotics (four permanent researchers) Up to 2014, I have been in charge of the activities about learning in the Robotics and Neuroscience group of the french working group in Robotics . I have been the principal investigator of the MACSi project (up to april 2014) based on our iCub humanoid robot. I also participate to the CODYCO and the DREAM european projects. Teaching \ud83d\udd17 A page about my teaching activities, with slides, tutorial texts and internship topics is available here .","title":"O. Sigaud"},{"location":"olivier-sigaud.html#olivier-sigaud","text":"","title":"Olivier Sigaud"},{"location":"olivier-sigaud.html#biography","text":"(1996) PhD in Computer Science \"Learning : from control to behavior\", advisor: Dominique Luzeaux (DGA) (1995-2001) Research engineer, Departement for Advanced Studies, at DASSAULT AVIATION (Aerospace company). (2001-2005) Lecturer in Computer Science UPMC-Paris6 (LIP6, AnimatLab) (2002) PhD in Philosophy \" Automatisme et subjectivit\u00e9 : l'anticipation au coeur de l'exp\u00e9rience\", advisor: Jacques Dubucs (IHSPST) (2004) HDR in Computer Science \"Adaptive Behavior for Agents in Complex Software Settings\" (in french), UPMC-Paris 6 (2005-2006) Professor in Computer Science UPMC-Paris6 (LIP6, AnimatLab) (2007- ) Professor in Computer Science UPMC-Paris6 (ISIR, AMAC team)","title":"Biography"},{"location":"olivier-sigaud.html#research-activities","text":"Understanding intelligence is a fascinating research topic. For a long time, artificial intelligence researchers have focused on typically human intellectual activities such as playing chess, having a conversation or proving theorems. But it is more and more obvious that these specific capabilities are anchored into apparently simpler capabilities such as using objects or tools, displaying social signals through the body posture or more simply moving one's body to interact appropriately with the environment. Performing robotics research reveals the difficulty of these problems that we solve effortlessly. Rather than addressing them through the standard engineering approach, it seems appropriate to try to figure out how our brain solves them. This amounts to investigating the psychological mechanisms through which kids acquire their motor and cognitive capabilities and neurophysiological mechanisms the brain uses to implement this learning process. In this context, my research is focused on designing and exploiting machine learning techniques dedicated to growing motor and cognitive capabilities in robots, and to modelling these capabilities in living beings. In practice, my work belongs to three domains: machine learning, where I'm interested in regression, reinforcement learning, stochastic optimisation and the combination of these methods. This domain provides the fundamental tools for the other two activities; developmental robotics, which strive to endow robots with psychological mechanisms similar to those of kids; computational neurosciences, which strive to design computational models of the neurophysiological mechanism of animal learning. Within ISIR, I am responsible of a group about Learning for Control and Decision in Robotics (four permanent researchers) Up to 2014, I have been in charge of the activities about learning in the Robotics and Neuroscience group of the french working group in Robotics . I have been the principal investigator of the MACSi project (up to april 2014) based on our iCub humanoid robot. I also participate to the CODYCO and the DREAM european projects.","title":"Research Activities"},{"location":"olivier-sigaud.html#teaching","text":"A page about my teaching activities, with slides, tutorial texts and internship topics is available here .","title":"Teaching"},{"location":"organizers.html","text":"Organizers \ud83d\udd17 Program committee David Bertoin IRT Saint Exup\u00e9ry Tommaso Cesari Toulouse School of Economics S\u00e9bastien Gerchinovitz IRT Saint Exup\u00e9ry and Institut de Math\u00e9matiques de Toulouse Nicolas Mansard LAAS-CNRS Emmanuel Rachelson ISAE-SUPAERO Local arrangement chair Corinne Joffre Universit\u00e9 de Toulouse Communication chair Alix Fauque de Jonqui\u00e8res Universit\u00e9 de Toulouse","title":"Organizers"},{"location":"organizers.html#organizers","text":"Program committee David Bertoin IRT Saint Exup\u00e9ry Tommaso Cesari Toulouse School of Economics S\u00e9bastien Gerchinovitz IRT Saint Exup\u00e9ry and Institut de Math\u00e9matiques de Toulouse Nicolas Mansard LAAS-CNRS Emmanuel Rachelson ISAE-SUPAERO Local arrangement chair Corinne Joffre Universit\u00e9 de Toulouse Communication chair Alix Fauque de Jonqui\u00e8res Universit\u00e9 de Toulouse","title":"Organizers"},{"location":"pg-pitfalls.html","text":"Pitfalls in Policy Gradient methods \ud83d\udd17 Abstract \ud83d\udd17 In this talk, I will present the behavior of variants of the REINFORCE algorithm using simple gym classic control benchmarks (CartPole, Pendulum, MountainCar...) and various stochastic policy representations (Bernoulli, Gaussian, squashed Gaussian). I will highlight difficulties faced by these algorithms on those simple environments and draw lessons about the necessity to better understanding how they work or why they don't before moving to more advanced methods and more complex benchmarks.","title":"Pitfalls in Policy Gradient methods"},{"location":"pg-pitfalls.html#pitfalls-in-policy-gradient-methods","text":"","title":"Pitfalls in Policy Gradient methods"},{"location":"pg-pitfalls.html#abstract","text":"In this talk, I will present the behavior of variants of the REINFORCE algorithm using simple gym classic control benchmarks (CartPole, Pendulum, MountainCar...) and various stochastic policy representations (Bernoulli, Gaussian, squashed Gaussian). I will highlight difficulties faced by these algorithms on those simple environments and draw lessons about the necessity to better understanding how they work or why they don't before moving to more advanced methods and more complex benchmarks.","title":"Abstract"},{"location":"pg.html","text":"From Policy Gradients to Actor Critic methods \ud83d\udd17 Abstract \ud83d\udd17 Starting from the general policy search problem and direct policy search methods, I will give a didactical presentation of the Policy Gradient Theorem and explain some variants of the REINFORCE algorithm. From there, I will move step by step to presenting more advanced methods such as TRPO, PPO and Actor-Critic methods such as DDPG and SAC. Outline \ud83d\udd17 The policy search problem Policy search methods: direct policy search versus policy gradient Policy gradient derivation Understanding the policy gradient From policy gradient with baseline to actor-critic Bias-variance trade-off On-policy vs off-policy TRPO, ACKTR and PPO DDPG and TD3 SAC Wrap-up","title":"Policy Gradients and Actor Critic methods"},{"location":"pg.html#from-policy-gradients-to-actor-critic-methods","text":"","title":"From Policy Gradients to Actor Critic methods"},{"location":"pg.html#abstract","text":"Starting from the general policy search problem and direct policy search methods, I will give a didactical presentation of the Policy Gradient Theorem and explain some variants of the REINFORCE algorithm. From there, I will move step by step to presenting more advanced methods such as TRPO, PPO and Actor-Critic methods such as DDPG and SAC.","title":"Abstract"},{"location":"pg.html#outline","text":"The policy search problem Policy search methods: direct policy search versus policy gradient Policy gradient derivation Understanding the policy gradient From policy gradient with baseline to actor-critic Bias-variance trade-off On-policy vs off-policy TRPO, ACKTR and PPO DDPG and TD3 SAC Wrap-up","title":"Outline"},{"location":"regularized-mdps.html","text":"Regularized MDPs \ud83d\udd17","title":"Regularized MDPs"},{"location":"regularized-mdps.html#regularized-mdps","text":"","title":"Regularized MDPs"},{"location":"rl-fundamentals.html","text":"Reinforcement Learning fundamentals \ud83d\udd17 The overall goal of this session is to provide the RLVS participants with an overview of RL and an understanding of (some) key challenges. It should also introduce participants to issues that will be tackled in more details in the following classes. We will follow the course of a notebook together and we will alternate between a \"lecture\" mode and small illustrative exercices (with corrections) that the participants will have time to try themselves. We will introduce the modeling fundamentals of Makov Decision Processes. Then we will move our way up from toy examples and fundamental algorithms to key challenges in RL. In particular, we will focus on three key structuring issues for the RL practitioner: the exploration/exploitation tradeoff, value function approximation, and optimality search. Along the session we will link each topic to the corresponding classes in RLVS. Notebook","title":"RL fundamentals"},{"location":"rl-fundamentals.html#reinforcement-learning-fundamentals","text":"The overall goal of this session is to provide the RLVS participants with an overview of RL and an understanding of (some) key challenges. It should also introduce participants to issues that will be tackled in more details in the following classes. We will follow the course of a notebook together and we will alternate between a \"lecture\" mode and small illustrative exercices (with corrections) that the participants will have time to try themselves. We will introduce the modeling fundamentals of Makov Decision Processes. Then we will move our way up from toy examples and fundamental algorithms to key challenges in RL. In particular, we will focus on three key structuring issues for the RL practitioner: the exploration/exploitation tradeoff, value function approximation, and optimality search. Along the session we will link each topic to the corresponding classes in RLVS. Notebook","title":"Reinforcement Learning fundamentals"},{"location":"rlvs-overview.html","text":"Overview of the 2021 Reinforcement Learning Virtual School \ud83d\udd17 This short introduction will present the organization of RLVS 2021 and the progression of ideas across days and sessions. Presentation slides","title":"RLVS overview"},{"location":"rlvs-overview.html#overview-of-the-2021-reinforcement-learning-virtual-school","text":"This short introduction will present the organization of RLVS 2021 and the progression of ideas across days and sessions. Presentation slides","title":"Overview of the 2021 Reinforcement Learning Virtual School"},{"location":"sebastian-risi.html","text":"Sebastian Risi \ud83d\udd17 I am an Artificial Intelligence researcher that aims to make machines more adaptive and creative. My research is focused on computational evolution, deep learning, and crowdsourcing, with applications in robotics, video games, design, and art. My research asks questions such as: Can we create lifelong learning machines that continuously acquire new knowledge and skills? Can we create machines that learn from and work together with humans to solve tasks that neither humans nor machines can solve by themselves? I co-direct the Robotics, Evolution and Art Lab (REAL) at the IT University of Copenhagen , in which we explore such questions and are establishing new foundations for future robotics and other artificial cognitive systems. I am also one of the co-founders of modl.ai , a company that develops AI techniques for game development. Before joining the IT University I was a postdoctoral researcher in Hod Lipson\u2019s Creative Machines Lab at Cornell University. From 2008-2012, I was a PhD student in the Evolutionary Complexity Research Group at the University of Central Florida, headed by Kenneth Stanley. Our research has been covered in different news outlets such as Science , New Scientist , Wired , Popular Mechanics , The Register , and Popular Science . http://sebastianrisi.com/","title":"S. Risi"},{"location":"sebastian-risi.html#sebastian-risi","text":"I am an Artificial Intelligence researcher that aims to make machines more adaptive and creative. My research is focused on computational evolution, deep learning, and crowdsourcing, with applications in robotics, video games, design, and art. My research asks questions such as: Can we create lifelong learning machines that continuously acquire new knowledge and skills? Can we create machines that learn from and work together with humans to solve tasks that neither humans nor machines can solve by themselves? I co-direct the Robotics, Evolution and Art Lab (REAL) at the IT University of Copenhagen , in which we explore such questions and are establishing new foundations for future robotics and other artificial cognitive systems. I am also one of the co-founders of modl.ai , a company that develops AI techniques for game development. Before joining the IT University I was a postdoctoral researcher in Hod Lipson\u2019s Creative Machines Lab at Cornell University. From 2008-2012, I was a PhD student in the Evolutionary Complexity Research Group at the University of Central Florida, headed by Kenneth Stanley. Our research has been covered in different news outlets such as Science , New Scientist , Wired , Popular Mechanics , The Register , and Popular Science . http://sebastianrisi.com/","title":"Sebastian Risi"},{"location":"sebastien-gerchinovitz.html","text":"Sebastien Gerchinovitz \ud83d\udd17","title":"Sebastien Gerchinovitz"},{"location":"sebastien-gerchinovitz.html#sebastien-gerchinovitz","text":"","title":"Sebastien Gerchinovitz"},{"location":"stochastic-bandits.html","text":"Stochastic bandits \ud83d\udd17","title":"Stochastic bandits"},{"location":"stochastic-bandits.html#stochastic-bandits","text":"","title":"Stochastic bandits"},{"location":"tips-and-tricks.html","text":"RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3 \ud83d\udd17 Abstract \ud83d\udd17 The aim of the session is to help you do reinforcement learning experiments. The first part covers general advice about RL, tips and tricks and details three examples where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library. Pre-requisites \ud83d\udd17 Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab). Additional material \ud83d\udd17 Website: https://github.com/DLR-RM/stable-baselines3 Doc: https://stable-baselines3.readthedocs.io/en/master/ Outline \ud83d\udd17 Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots Introduction (3 minutes) RL Tips and tricks (45 minutes) General Nuts and Bolt of RL experimentation (10 minutes) RL in practice on a custom task (custom environment) (30 minutes) Questions? (5 minutes) The Challenges of Applying RL to Real Robots (45 minutes) Learning to control an elastic robot - DLR David Neck Example (15 minutes) Learning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes) Learning to walk with an elastic quadruped robot - DLR bert example (10 minutes) Questions? (5 minutes+) Part II: Practical Session with Stable-Baselines3 Stable-Baselines3 Overview (20 minutes) Questions? (5 minutes) Practical Session - Code along (1h+)","title":"RL tips and tricks"},{"location":"tips-and-tricks.html#rl-in-practice-tips-and-tricks-and-practical-session-with-stable-baselines3","text":"","title":"RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3"},{"location":"tips-and-tricks.html#abstract","text":"The aim of the session is to help you do reinforcement learning experiments. The first part covers general advice about RL, tips and tricks and details three examples where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library.","title":"Abstract"},{"location":"tips-and-tricks.html#pre-requisites","text":"Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab).","title":"Pre-requisites"},{"location":"tips-and-tricks.html#additional-material","text":"Website: https://github.com/DLR-RM/stable-baselines3 Doc: https://stable-baselines3.readthedocs.io/en/master/","title":"Additional material"},{"location":"tips-and-tricks.html#outline","text":"Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots Introduction (3 minutes) RL Tips and tricks (45 minutes) General Nuts and Bolt of RL experimentation (10 minutes) RL in practice on a custom task (custom environment) (30 minutes) Questions? (5 minutes) The Challenges of Applying RL to Real Robots (45 minutes) Learning to control an elastic robot - DLR David Neck Example (15 minutes) Learning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes) Learning to walk with an elastic quadruped robot - DLR bert example (10 minutes) Questions? (5 minutes+) Part II: Practical Session with Stable-Baselines3 Stable-Baselines3 Overview (20 minutes) Questions? (5 minutes) Practical Session - Code along (1h+)","title":"Outline"},{"location":"tommaso-cesari.html","text":"Tommaso Cesari \ud83d\udd17","title":"Tommaso Cesari"},{"location":"tommaso-cesari.html#tommaso-cesari","text":"","title":"Tommaso Cesari"},{"location":"tor-lattimore.html","text":"Tor Lattimore \ud83d\udd17 I am a research scientist at DeepMind in London, working mostly on algorithms for sequential decision making. https://tor-lattimore.com/","title":"T. Lattimore"},{"location":"tor-lattimore.html#tor-lattimore","text":"I am a research scientist at DeepMind in London, working mostly on algorithms for sequential decision making. https://tor-lattimore.com/","title":"Tor Lattimore"}]}