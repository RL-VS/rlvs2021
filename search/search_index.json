{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"RLVS 2021 \ud83d\udd17 Welcome to the 2021 Reinforcement Learning Virtual School's website. This edition is hosted by ANITI . Event website: https://rlvs.aniti.fr . Goals of RLVS \ud83d\udd17 RLVS intends to provide a high-quality, easy access to the field of Reinforcement Learning to new researchers. It aims to: - Allow attendees to step in to the field of RL - Learn from top lecturers in the field - Provide broad access to live classes We expect the participants to be a mixture of masters and Ph.D. students, academics, and industrial researchers with a solid background in mathematics and computer science. RLVS schedule \ud83d\udd17 This condensed schedule does not include class breaks and social events. Schedule March 25th 9:00-9:30 RLVS Overview E. Rachelson 9:30-12:30 RL fundamentals E. Rachelson 14:00-16:00 Deep Learning D. Wilson 16:30-17:30 Human behavioral agents I. Rish March 26th 10:00-12:00 Stochastic bandits T. Lattimore 14:00-16:00 Monte Carlo Tree Search T. Lattimore 16:30-17:30 Multi-armed bandits in clinical trials D. A. Berry April 1st 9:00-15:00 Deep Q-Networks and its variants B. Piot 15:15-16:15 Regularized MDPs M. Geist 16:30-17:30 TBA M. Wang April 2nd 9:00-12:30 Policy Gradients and Actor Critic methods O. Sigaud 14:00-15:00 Pitfalls in Policy Gradient methods O. Sigaud 15:30-17:30 Exploration in Deep RL M. Pirotta April 8th 9:00-11:00 Evolutionary Reinforcement Learning D. Wilson , J.-B. Mouret 11:30-12:30 TBA S. Risi 14:00-16:00 Micro-data Policy Search K. Chatzilygeroudis, J.-B. Mouret 14:00-16:00 TBA April 9th 9:00-13:00 RL tips and tricks A. Raffin 14:30-15:30 TBA M. Garnelo 15:45-16:45 TBA L. P. Kaelbling 17:00-18:00 RLVS wrap-up E. Rachelson Speakers \ud83d\udd17 Sorted alphabetically Donald A. Berry University of Texas and Rice University Emmanuel Rachelson ISAE-SUPAERO, Universit\u00e9 de Toulouse Irina Rish Universit\u00e9 de Montr\u00e9al Dennis Wilson ISAE-SUPAERO, Universit\u00e9 de Toulouse Organizers \ud83d\udd17","title":"RLVS home"},{"location":"index.html#rlvs-2021","text":"Welcome to the 2021 Reinforcement Learning Virtual School's website. This edition is hosted by ANITI . Event website: https://rlvs.aniti.fr .","title":"RLVS 2021"},{"location":"index.html#goals-of-rlvs","text":"RLVS intends to provide a high-quality, easy access to the field of Reinforcement Learning to new researchers. It aims to: - Allow attendees to step in to the field of RL - Learn from top lecturers in the field - Provide broad access to live classes We expect the participants to be a mixture of masters and Ph.D. students, academics, and industrial researchers with a solid background in mathematics and computer science.","title":"Goals of RLVS"},{"location":"index.html#rlvs-schedule","text":"This condensed schedule does not include class breaks and social events. Schedule March 25th 9:00-9:30 RLVS Overview E. Rachelson 9:30-12:30 RL fundamentals E. Rachelson 14:00-16:00 Deep Learning D. Wilson 16:30-17:30 Human behavioral agents I. Rish March 26th 10:00-12:00 Stochastic bandits T. Lattimore 14:00-16:00 Monte Carlo Tree Search T. Lattimore 16:30-17:30 Multi-armed bandits in clinical trials D. A. Berry April 1st 9:00-15:00 Deep Q-Networks and its variants B. Piot 15:15-16:15 Regularized MDPs M. Geist 16:30-17:30 TBA M. Wang April 2nd 9:00-12:30 Policy Gradients and Actor Critic methods O. Sigaud 14:00-15:00 Pitfalls in Policy Gradient methods O. Sigaud 15:30-17:30 Exploration in Deep RL M. Pirotta April 8th 9:00-11:00 Evolutionary Reinforcement Learning D. Wilson , J.-B. Mouret 11:30-12:30 TBA S. Risi 14:00-16:00 Micro-data Policy Search K. Chatzilygeroudis, J.-B. Mouret 14:00-16:00 TBA April 9th 9:00-13:00 RL tips and tricks A. Raffin 14:30-15:30 TBA M. Garnelo 15:45-16:45 TBA L. P. Kaelbling 17:00-18:00 RLVS wrap-up E. Rachelson","title":"RLVS schedule"},{"location":"index.html#speakers","text":"Sorted alphabetically Donald A. Berry University of Texas and Rice University Emmanuel Rachelson ISAE-SUPAERO, Universit\u00e9 de Toulouse Irina Rish Universit\u00e9 de Montr\u00e9al Dennis Wilson ISAE-SUPAERO, Universit\u00e9 de Toulouse","title":"Speakers"},{"location":"index.html#organizers","text":"","title":"Organizers"},{"location":"dennis-wilson.html","text":"Dennis Wilson \ud83d\udd17 I am an assistant professor in AI and Data Science at ISAE-SUPAERO . My research focuses on bio-inspired artificial intelligence, namely genetic programming, neural networks, and the evolution of learning. I obtained my PhD at the Institut de Recherche en Informatique de Toulouse (IRIT) on the evolution of design principles for artificial neural networks. Prior to that, I worked in the Anyscale Learning For All group in CSAIL, MIT, applying evolutionary strategies and developmental models to the problem of wind farm layout optimization. I\u2019m currently co-organizer of the Supaero Reinforcement Learning Initiative ( SuReLi ), head of the Data & Decision Sciences Master\u2019s program, and am a part of the Decision Systems research group in the Department of Complex Systems Engineering.","title":"D. Wilson"},{"location":"dennis-wilson.html#dennis-wilson","text":"I am an assistant professor in AI and Data Science at ISAE-SUPAERO . My research focuses on bio-inspired artificial intelligence, namely genetic programming, neural networks, and the evolution of learning. I obtained my PhD at the Institut de Recherche en Informatique de Toulouse (IRIT) on the evolution of design principles for artificial neural networks. Prior to that, I worked in the Anyscale Learning For All group in CSAIL, MIT, applying evolutionary strategies and developmental models to the problem of wind farm layout optimization. I\u2019m currently co-organizer of the Supaero Reinforcement Learning Initiative ( SuReLi ), head of the Data & Decision Sciences Master\u2019s program, and am a part of the Decision Systems research group in the Department of Complex Systems Engineering.","title":"Dennis Wilson"},{"location":"dqn.html","text":"Deep Q-Networks and its variants \ud83d\udd17 Abstract \ud83d\udd17 We will present in a coherent paradigm the different breakthroughs that led to the seminal paper Deep Q-Networks (DQN). Starting from dynamic programming and the value iteration algorithm, we will show how DQN can be seen as a particular instance of an approximate value iteration algorithm. Then, we will present an open-source codebase released by DeepMind called dqn_zoo that implements in JAX the DQN algorithm. Finally, we will present the variants of DQN, from Double DQN (DDQN) to Implicite Quantile DQN (IQN) and more. Prerequisites: \ud83d\udd17 Understanding of Markov Decisions Processes and Bellman equations (starting point of the presentation) Basic notions of Python and JAX. (Optional) Run the dqn_zoo codebase.","title":"Deep Q-Networks and its variants"},{"location":"dqn.html#deep-q-networks-and-its-variants","text":"","title":"Deep Q-Networks and its variants"},{"location":"dqn.html#abstract","text":"We will present in a coherent paradigm the different breakthroughs that led to the seminal paper Deep Q-Networks (DQN). Starting from dynamic programming and the value iteration algorithm, we will show how DQN can be seen as a particular instance of an approximate value iteration algorithm. Then, we will present an open-source codebase released by DeepMind called dqn_zoo that implements in JAX the DQN algorithm. Finally, we will present the variants of DQN, from Double DQN (DDQN) to Implicite Quantile DQN (IQN) and more.","title":"Abstract"},{"location":"dqn.html#prerequisites","text":"Understanding of Markov Decisions Processes and Bellman equations (starting point of the presentation) Basic notions of Python and JAX. (Optional) Run the dqn_zoo codebase.","title":"Prerequisites:"},{"location":"emmanuel-rachelson.html","text":"Emmanuel Rachelson \ud83d\udd17 I am a professor of Machine Learning and Optimization at ISAE-SUPAERO . My research lies in the fields of Reinforcement Learning and Sequential Decision problems, with a broader interest for Machine Learning and Operations Research. I am the part of the \"Decision Systems\" research group within the Department of Complex Systems Engineering (DISC) and the founder and co-head of SuReLI (Supaero Reinforcement Learning Initiative). I created the Data & Decision Sciences MS curriculum and am implied more generally on (most) AI-related topics within the cursus. My website: https://personnel.isae-supaero.fr/emmanuel-rachelson","title":"E. Rachelson"},{"location":"emmanuel-rachelson.html#emmanuel-rachelson","text":"I am a professor of Machine Learning and Optimization at ISAE-SUPAERO . My research lies in the fields of Reinforcement Learning and Sequential Decision problems, with a broader interest for Machine Learning and Operations Research. I am the part of the \"Decision Systems\" research group within the Department of Complex Systems Engineering (DISC) and the founder and co-head of SuReLI (Supaero Reinforcement Learning Initiative). I created the Data & Decision Sciences MS curriculum and am implied more generally on (most) AI-related topics within the cursus. My website: https://personnel.isae-supaero.fr/emmanuel-rachelson","title":"Emmanuel Rachelson"},{"location":"exploration.html","text":"Exploration-Exploitation in Reinforcement Learning: Function Approximation and Deep RL \ud83d\udd17 One of the major challenges in reinforcement learning (RL) is the trade-off between exploration of the environment to gather information and exploitation of the samples observed so far to execute \"good\" (nearly optimal) actions. In this seminar, we review how exploration techniques are paired with function approximation in continuous state-action spaces. In particular, we will focus on the integration of exploration mechanisms with deep learning techniques. The seminar should provide enough theoretical and algorithmic background to understand existing techniques and possibly devise novel methods. Throughout the talk, we will discuss open problems and possible future research directions.","title":"Exploration in Deep RL"},{"location":"exploration.html#exploration-exploitation-in-reinforcement-learning-function-approximation-and-deep-rl","text":"One of the major challenges in reinforcement learning (RL) is the trade-off between exploration of the environment to gather information and exploitation of the samples observed so far to execute \"good\" (nearly optimal) actions. In this seminar, we review how exploration techniques are paired with function approximation in continuous state-action spaces. In particular, we will focus on the integration of exploration mechanisms with deep learning techniques. The seminar should provide enough theoretical and algorithmic background to understand existing techniques and possibly devise novel methods. Throughout the talk, we will discuss open problems and possible future research directions.","title":"Exploration-Exploitation in Reinforcement Learning: Function Approximation and Deep RL"},{"location":"matteo-pirotta.html","text":"Matteo Pirotta \ud83d\udd17","title":"Matteo Pirotta"},{"location":"matteo-pirotta.html#matteo-pirotta","text":"","title":"Matteo Pirotta"},{"location":"olivier-sigaud.html","text":"Olivier Sigaud \ud83d\udd17","title":"O. Sigaud"},{"location":"olivier-sigaud.html#olivier-sigaud","text":"","title":"Olivier Sigaud"},{"location":"organizers.html","text":"Organizers \ud83d\udd17 That's us!","title":"Organizers"},{"location":"organizers.html#organizers","text":"That's us!","title":"Organizers"},{"location":"pg-pitfalls.html","text":"Pitfalls in Policy Gradient methods \ud83d\udd17 Abstract \ud83d\udd17 In this talk, I will present the behavior of variants of the REINFORCE algorithm using simple gym classic control benchmarks (CartPole, Pendulum, MountainCar...) and various stochastic policy representations (Bernoulli, Gaussian, squashed Gaussian). I will highlight difficulties faced by these algorithms on those simple environments and draw lessons about the necessity to better understanding how they work or why they don't before moving to more advanced methods and more complex benchmarks.","title":"Pitfalls in Policy Gradient methods"},{"location":"pg-pitfalls.html#pitfalls-in-policy-gradient-methods","text":"","title":"Pitfalls in Policy Gradient methods"},{"location":"pg-pitfalls.html#abstract","text":"In this talk, I will present the behavior of variants of the REINFORCE algorithm using simple gym classic control benchmarks (CartPole, Pendulum, MountainCar...) and various stochastic policy representations (Bernoulli, Gaussian, squashed Gaussian). I will highlight difficulties faced by these algorithms on those simple environments and draw lessons about the necessity to better understanding how they work or why they don't before moving to more advanced methods and more complex benchmarks.","title":"Abstract"},{"location":"pg.html","text":"From Policy Gradients to Actor Critic methods \ud83d\udd17 Abstract \ud83d\udd17 Starting from the general policy search problem and direct policy search methods, I will give a didactical presentation of the Policy Gradient Theorem and explain some variants of the REINFORCE algorithm. From there, I will move step by step to presenting more advanced methods such as TRPO, PPO and Actor-Critic methods such as DDPG and SAC. Outline \ud83d\udd17 The policy search problem Policy search methods: direct policy search versus policy gradient Policy gradient derivation Understanding the policy gradient From policy gradient with baseline to actor-critic Bias-variance trade-off On-policy vs off-policy TRPO, ACKTR and PPO DDPG and TD3 SAC Wrap-up","title":"Policy Gradients and Actor Critic methods"},{"location":"pg.html#from-policy-gradients-to-actor-critic-methods","text":"","title":"From Policy Gradients to Actor Critic methods"},{"location":"pg.html#abstract","text":"Starting from the general policy search problem and direct policy search methods, I will give a didactical presentation of the Policy Gradient Theorem and explain some variants of the REINFORCE algorithm. From there, I will move step by step to presenting more advanced methods such as TRPO, PPO and Actor-Critic methods such as DDPG and SAC.","title":"Abstract"},{"location":"pg.html#outline","text":"The policy search problem Policy search methods: direct policy search versus policy gradient Policy gradient derivation Understanding the policy gradient From policy gradient with baseline to actor-critic Bias-variance trade-off On-policy vs off-policy TRPO, ACKTR and PPO DDPG and TD3 SAC Wrap-up","title":"Outline"},{"location":"rl-fundamentals.html","text":"Reinforcement Learning fundamentals \ud83d\udd17 The overall goal of this session is to provide the RLVS participants with an overview of RL and an understanding of (some) key challenges. It should also introduce participants to issues that will be tackled in more details in the following classes. We will follow the course of a notebook together and we will alternate between a \"lecture\" mode and small illustrative exercices (with corrections) that the participants will have time to try themselves. We will introduce the modeling fundamentals of Makov Decision Processes. Then we will move our way up from toy examples and fundamental algorithms to key challenges in RL. In particular, we will focus on three key structuring issues for the RL practitioner: the exploration/exploitation tradeoff, value function approximation, and optimality search. Along the session we will link each topic to the corresponding classes in RLVS. Notebook","title":"RL fundamentals"},{"location":"rl-fundamentals.html#reinforcement-learning-fundamentals","text":"The overall goal of this session is to provide the RLVS participants with an overview of RL and an understanding of (some) key challenges. It should also introduce participants to issues that will be tackled in more details in the following classes. We will follow the course of a notebook together and we will alternate between a \"lecture\" mode and small illustrative exercices (with corrections) that the participants will have time to try themselves. We will introduce the modeling fundamentals of Makov Decision Processes. Then we will move our way up from toy examples and fundamental algorithms to key challenges in RL. In particular, we will focus on three key structuring issues for the RL practitioner: the exploration/exploitation tradeoff, value function approximation, and optimality search. Along the session we will link each topic to the corresponding classes in RLVS. Notebook","title":"Reinforcement Learning fundamentals"},{"location":"rlvs-overview.html","text":"Overview of the 2021 Reinforcement Learning Virtual School \ud83d\udd17 This short introduction will present the organization of RLVS 2021 and the progression of ideas across days and sessions. Presentation slides","title":"RLVS overview"},{"location":"rlvs-overview.html#overview-of-the-2021-reinforcement-learning-virtual-school","text":"This short introduction will present the organization of RLVS 2021 and the progression of ideas across days and sessions. Presentation slides","title":"Overview of the 2021 Reinforcement Learning Virtual School"},{"location":"tips-and-tricks.html","text":"RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3 \ud83d\udd17 Abstract \ud83d\udd17 The aim of the session is to help you do reinforcement learning experiments. The first part covers general advice about RL, tips and tricks and details three examples where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library. Pre-requisites \ud83d\udd17 Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab). Additional material \ud83d\udd17 Website: https://github.com/DLR-RM/stable-baselines3 Doc: https://stable-baselines3.readthedocs.io/en/master/ Outline \ud83d\udd17 Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots Introduction (3 minutes) RL Tips and tricks (45 minutes) General Nuts and Bolt of RL experimentation (10 minutes) RL in practice on a custom task (custom environment) (30 minutes) Questions? (5 minutes) The Challenges of Applying RL to Real Robots (45 minutes) Learning to control an elastic robot - DLR David Neck Example (15 minutes) Learning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes) Learning to walk with an elastic quadruped robot - DLR bert example (10 minutes) Questions? (5 minutes+) Part II: Practical Session with Stable-Baselines3 Stable-Baselines3 Overview (20 minutes) Questions? (5 minutes) Practical Session - Code along (1h+)","title":"RL tips and tricks"},{"location":"tips-and-tricks.html#rl-in-practice-tips-and-tricks-and-practical-session-with-stable-baselines3","text":"","title":"RL in Practice: Tips and Tricks and Practical Session With Stable-Baselines3"},{"location":"tips-and-tricks.html#abstract","text":"The aim of the session is to help you do reinforcement learning experiments. The first part covers general advice about RL, tips and tricks and details three examples where RL was applied on real robots. The second part will be a practical session using the Stable-Baselines3 library.","title":"Abstract"},{"location":"tips-and-tricks.html#pre-requisites","text":"Python programming, RL basics, (recommended: Google account for the practical session in order to use Google Colab).","title":"Pre-requisites"},{"location":"tips-and-tricks.html#additional-material","text":"Website: https://github.com/DLR-RM/stable-baselines3 Doc: https://stable-baselines3.readthedocs.io/en/master/","title":"Additional material"},{"location":"tips-and-tricks.html#outline","text":"Part I: RL Tips and Tricks / The Challenges of Applying RL to Real Robots Introduction (3 minutes) RL Tips and tricks (45 minutes) General Nuts and Bolt of RL experimentation (10 minutes) RL in practice on a custom task (custom environment) (30 minutes) Questions? (5 minutes) The Challenges of Applying RL to Real Robots (45 minutes) Learning to control an elastic robot - DLR David Neck Example (15 minutes) Learning to drive in minutes and learning to race in hours - Virtual and real racing car (15 minutes) Learning to walk with an elastic quadruped robot - DLR bert example (10 minutes) Questions? (5 minutes+) Part II: Practical Session with Stable-Baselines3 Stable-Baselines3 Overview (20 minutes) Questions? (5 minutes) Practical Session - Code along (1h+)","title":"Outline"}]}